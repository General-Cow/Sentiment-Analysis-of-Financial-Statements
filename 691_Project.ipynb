{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Financial Statements\n",
        "\n",
        "The goal for our project was be to take the finance phrasebank data put together by Malo, Pekka, et al. (2013). Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts. and develop a deep learning/neural network pipeline to classify the sentiment of the sentences in the dataset. The goal was to benchmark our results against the results of Malo et al., particularly the LPS method they developed.\n",
        "\n",
        "The NLP task we used in this project was sentiment analysis in which we attempting to classified different sentences as positive, negative, or neutral.\n",
        "\n",
        "To do so we will use the GloVe premade word vectors to embed the words. We also implemented 2 neural networks for this project; a sequence-to-sequence encoder-decoder GRU based model and a basic GRU model.\n",
        "\n",
        "Below we link to the arxiv page for Malo et al's paper\n",
        "\n",
        "https://arxiv.org/pdf/1307.5336"
      ],
      "metadata": {
        "id": "82FmKHag-63i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Team Members\n",
        "\n",
        "- Group member 1\n",
        "    - Name: David Blankenkship\n",
        "    - Email: dwb65@drexel.edu\n",
        "- Group member 2\n",
        "    - Name: Christian Ekwomadu\n",
        "    - Email: cce49@drexel.edu\n",
        "- Group member 3\n",
        "    - Name: Jai Vaidya\n",
        "    - Email: jv625@drexel.edu\n",
        "- Group member 4\n",
        "    - Name: Nana Afua Martinson\n",
        "    - Email: nsm86@drexel.edu"
      ],
      "metadata": {
        "id": "UmdbfisWk0OE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing/Building the LM\n",
        "\n",
        "The goal here is to build the datasets and embedding matrix using the Glove pretrained model and financial phrasebank data."
      ],
      "metadata": {
        "id": "YWbbhSJ7pBvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of our data\n",
        "test = 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral'\n",
        "x, y = test.split('@')\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzSSlbIs-ktV",
        "outputId": "ce819bfa-d33d-4890-b624-f0ec07938b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Statements"
      ],
      "metadata": {
        "id": "7sVVYHmMxB5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as ra\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "yoT8SCG4bQGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load GLOVE Pretrained Vectors"
      ],
      "metadata": {
        "id": "cMwSK8e1rQid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Glove Embeddings\n",
        "def load_glove_embeddings(glove_file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, encoding=\"utf-8\") as filename:\n",
        "        for line in filename:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "# Run as below\n",
        "# We'll use 6B 50d file at first in order to save on time.\n",
        "# glove_file_path = './glove.6B.50d.txt'\n",
        "# embeddings_index = load_glove_embeddings(glove_file_path)"
      ],
      "metadata": {
        "id": "cURJ0X3sRd74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Function\n",
        "Outputs X and y for test, train, and validation as well as embedding matrix."
      ],
      "metadata": {
        "id": "iV6wUbvtpDWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exec(open(\"01-utilities.py\").read()) # Will need to either put in here as files or call as code\n",
        "# Just putting tokenize() in here instead\n",
        "\n",
        "def tokenize(text, space = True):\n",
        "    tokens = []\n",
        "    for token in re.split(\"([0-9a-zA-Z'-]+)\", text):\n",
        "        if not space:\n",
        "            token = re.sub(\"[ ]+\", \"\", token)\n",
        "        if not token:\n",
        "            continue\n",
        "        if re.search(\"[0-9a-zA-Z'-]\", token):\n",
        "            tokens.append(token)\n",
        "        else:\n",
        "            tokens.extend(token)\n",
        "    return tokens\n",
        "\n",
        "#just using torchtext vocab instead\n",
        "#exec(open(\"05-utilities.py\").read()) # Will need to either put in here as files or call as code"
      ],
      "metadata": {
        "id": "pr50nSjXlEy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "\n",
        "  \"\"\"\n",
        "  Class that handles mapping to and from\n",
        "  words to vocab indices. Built off of class utilities file 5.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self._word2idx = {'<unk>': 0, '<pad>': 1}\n",
        "    self._idx2word = {}\n",
        "\n",
        "\n",
        "  def train(self, sentence_list):\n",
        "    # generate vocab list\n",
        "    for sentence in sentence_list:\n",
        "      for token in sentence:\n",
        "        if token not in self._word2idx:\n",
        "          self._word2idx[token] = len(self._word2idx)\n",
        "\n",
        "    # rebuild reverse lookup\n",
        "    self._idx2word = {v: k for k, v in self._word2idx.items()}\n",
        "\n",
        "  def encode(self, word):\n",
        "      return self._word2idx.get(word, self._word2idx['<unk>'])\n",
        "\n",
        "  # def decode(self, idx):\n",
        "  #   if self._target:\n",
        "  #     return self._idx2word[idx]\n",
        "  #   else:\n",
        "  #     return self._idx2word.get(idx, '<unk>')"
      ],
      "metadata": {
        "id": "84taQOMnWjzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finance_preprocessing(filename, embeddings_index, embed_dim=50):\n",
        "  with open(filename, mode='r', encoding='iso-8859-1') as file: # the phrasebank data uses 'iso-8859-1' or latin-1 encoding.\n",
        "    rows = [row.rstrip() for row in file]\n",
        "\n",
        "  # Create data dictionary and relabel sentiment as numbers\n",
        "  data={}\n",
        "  sent_list = []\n",
        "  label_list = []\n",
        "  for row in rows:\n",
        "    sentence, label = row.rsplit('@')\n",
        "    # Turns sentiment targets into numbers.\n",
        "    # I shifted to 0, 1, 2 as cross entropy loss would otherise require one hot encodings.\n",
        "    if label == 'neutral':\n",
        "      label_num = 1\n",
        "    elif label == 'positive':\n",
        "      label_num = 2\n",
        "    elif label == 'negative':\n",
        "      label_num = 0\n",
        "    sent_list.append(sentence)\n",
        "    label_list.append(label_num)\n",
        "  data['sentences'] = sent_list\n",
        "  data['label'] = label_list\n",
        "\n",
        "  # Tokenize sentences\n",
        "  max_sent_len = 0\n",
        "  tok_sent_list = []\n",
        "  for sent in data['sentences']:\n",
        "    s = tokenize(sent.lower(), space=False)\n",
        "    if len(s) > max_sent_len:\n",
        "      max_sent_len = len(s)\n",
        "    tok_sent_list.append(s)\n",
        "  data['sentences'] = tok_sent_list\n",
        "  print('Max Sentence length: ', max_sent_len)\n",
        "\n",
        "  # Develop Vocab using custom class\n",
        "  vocab = Vocab()\n",
        "  vocab.train(data['sentences'])\n",
        "\n",
        "\n",
        "  # Creates sequences by encoding the tokenized sentences\n",
        "  sequences = []\n",
        "  for sentence in data['sentences']:\n",
        "    sequences.append([vocab.encode(tok) for tok in sentence])\n",
        "\n",
        "\n",
        "  # pads out sentences to max length using the <pad token>\n",
        "  padded_sequences = [seq + [vocab.encode('<pad>')] * (max_sent_len - len(seq)) for seq in sequences]\n",
        "\n",
        "  # Create tensors of full data sets\n",
        "  X = torch.tensor(padded_sequences, dtype=torch.long)\n",
        "  y = torch.tensor(data['label'], dtype=torch.long)\n",
        "\n",
        "  # Split the data\n",
        "  X_tv, X_test, y_tv, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=691)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=0.20, stratify=y_tv, random_state=691)\n",
        "\n",
        "  # Creates embedding matrix\n",
        "  embedding_matrix = np.zeros((len(vocab._word2idx), embed_dim))\n",
        "  for word, idx in vocab._word2idx.items():\n",
        "      if word in embeddings_index:\n",
        "          embedding_matrix[idx] = embeddings_index[word]\n",
        "      else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.5, size=(embed_dim, )) # Randomized vector for not in glove\n",
        "\n",
        "  embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "7UeifLWTlapI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Dataset Class\n",
        "Needed to use DataLoader"
      ],
      "metadata": {
        "id": "zgu1-9vRfMtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data in the correct format\n",
        "        return {'sequences': self.sequences[idx], 'labels': self.labels[idx]}\n",
        "\n"
      ],
      "metadata": {
        "id": "Zg-upa8vfM_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Neural Network\n",
        "\n",
        "This is where we create the architecture of our Neural Network. The output for this entire step will be the sentiment of the sentence. We use the following models:\n",
        "\n",
        "1. Sequence-to-Sequence GRU-based Encoder-Decoder Model\n",
        "2. Basic GRU Model\n",
        "\n",
        "Both will include dropout, embedding matrices based off of the earlier GloVe premade word vectors, and variable hidden size and GRU layers."
      ],
      "metadata": {
        "id": "axNbwJn8jIK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU Encoder/Decoder"
      ],
      "metadata": {
        "id": "aIhY97m6xs0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderGRU(nn.Module):\n",
        "  def __init__(self, hidden_size, embedding_matrix, gru_layers = 1, dropout=0.1):\n",
        "    super(EncoderGRU, self).__init__()\n",
        "    self.hidden_size = hidden_size # hyperparam to tweak\n",
        "    self.embedding_size = embedding_matrix.shape[1]\n",
        "    self.num_layers = gru_layers # hyperparam to tweak\n",
        "    self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "    self.gru = nn.GRU(self.embedding_size, self.hidden_size,\n",
        "                      num_layers = self.num_layers, bidirectional = True,\n",
        "                      batch_first=True, dropout=dropout)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    embedded = self.embedding(input)\n",
        "    output, hidden = self.gru(embedded)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "SXgp6dIXfbTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderGRU(nn.Module):\n",
        "  def __init__(self, hidden_size, gru_layers=1, dropout=0.1):\n",
        "    super(DecoderGRU, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = gru_layers\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size,\n",
        "                      num_layers = self.num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(self.hidden_size, 3) # hardcode ouput size to 3. There's no ambiguity about output size\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, hidden):\n",
        "    output, _ = self.gru(hidden)\n",
        "    output = self.dropout(output)\n",
        "    output = self.fc(output.squeeze(1))\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "mHuk3G-sfgKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will be what we input as the model\n",
        "class GRU2GRU(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GRU2GRU, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input):\n",
        "        encoder_output, hidden = self.encoder(input)\n",
        "        output = self.decoder(hidden[-1])\n",
        "        return output\n",
        "\n",
        "# Called like so\n",
        "# encoder = EncoderGRU(embedding_matrix, hidden_size, gru_layers)\n",
        "# decoder = DecoderGRU(hidden_size, gru_layers)\n",
        "# model = GRU2GRU(encoder, decoder)"
      ],
      "metadata": {
        "id": "RAtUOUJ9QeLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic GRU Classifier\n",
        "\n",
        "Simple model to compare against our encoder-decoder GRUs"
      ],
      "metadata": {
        "id": "vH-i3eNePvFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_matrix, gru_layers=1, dropout=0.1):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_matrix.shape[1]\n",
        "        self.num_layers = gru_layers\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.gru = nn.GRU(self.embedding_size, self.hidden_size,\n",
        "                      num_layers = self.num_layers, bidirectional = True,\n",
        "                      batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, 3)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        gru_output, hidden = self.gru(embedded)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        output = self.fc(hidden)\n",
        "        return output"
      ],
      "metadata": {
        "id": "ntnrgfSNFaA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model/Optimization/Backpropagation\n",
        "\n",
        "This is where we set up our training function to do optimization and backpropagation.\n",
        "\n",
        "Hyperparameters include batch size, learning rate, epochs, and clip."
      ],
      "metadata": {
        "id": "PdOKzvDx_wLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "def train_model(model, train_data, val_data, batch_size=64, lr=0.01, epochs=10, clip=0.25, model_run=0):\n",
        "    # Setup optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=1) #should ignore pad token\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    best_val_loss = float('inf') # is THIS JACKING US UP?\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0\n",
        "\n",
        "        # Training loop\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
        "            sequences, labels = batch['sequences'], batch['labels']\n",
        "            optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sequences)\n",
        "            if torch.isnan(outputs).sum() > 0:\n",
        "                print(\"NaN detected in model output.\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            if torch.isnan(loss).sum() > 0:\n",
        "                print(\"NaN detected in loss computation.\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
        "                sequences, labels = batch['sequences'], batch['labels']\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(sequences)\n",
        "                if torch.isnan(outputs).sum() > 0:\n",
        "                    print(\"NaN detected in model output during validation.\")\n",
        "                    continue\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                if torch.isnan(loss).sum() > 0:\n",
        "                    print(\"NaN detected in validation loss computation.\")\n",
        "                    continue\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_loss < best_val_loss: # consider more chances\n",
        "            best_val_loss = val_loss\n",
        "            # name format epoch, loss, model run\n",
        "            # Model run is a variable for distinguishing which set of hyper parameters was used\n",
        "            # 0 is reserved for testing, defaults will be at 1.\n",
        "            torch.save(model.state_dict(), f'best_model_run{model_run}.pt')\n",
        "        else:\n",
        "            print(\"No improvement! Early stopping.\")\n",
        "            break\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "BgDzWW7j_wYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction/Evaluation\n"
      ],
      "metadata": {
        "id": "UTQedzutAYRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def evaluate_model(model, data_loader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            sequences, labels = batch['sequences'].to(device), batch['labels'].to(device)\n",
        "            outputs = model(sequences)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0.0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "    print('\\nClassification Report:\\n', classification_report(all_labels, all_preds, target_names=class_names, zero_division=0.0))\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "LgnZxZ7AzwpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Models\n",
        "\n",
        "This is where we run the models side-by-side. We tweak hyperparameters and compare performance to determine the best values for the full run."
      ],
      "metadata": {
        "id": "Oxxg0VxT1QM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.50d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)"
      ],
      "metadata": {
        "id": "QUMu0lQA1V8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = './Sentences_AllAgree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=50)"
      ],
      "metadata": {
        "id": "xXwBGHdLs1Ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7cd3b1-102e-409f-8052-71f1ca91c8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "0WqJ-QYvs099"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model run number\n",
        "model_run = 0"
      ],
      "metadata": {
        "id": "3ighoAOmYPDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Run"
      ],
      "metadata": {
        "id": "kGi_FLC4fuzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model run number\n",
        "model_run = 0\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=10\n",
        "clip=0.25\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "# print(\"Test Set Evaluation\")\n",
        "# evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "# print(\"Test Set Evaluation\")\n",
        "# evaluate_model(simple_model, test_loader, device, class_names)\n"
      ],
      "metadata": {
        "id": "gXCn6q-rs07Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111aa16f-4924-4ffd-856f-a23666bab190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:06<00:00,  7.09it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 25.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/10 - Train Loss: 35.9594, Val Loss: 7.6293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:09<00:00,  5.15it/s]\n",
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:00<00:00, 23.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Train Loss: 29.8402, Val Loss: 5.9394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 49/49 [00:09<00:00,  5.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/10: 100%|██████████| 13/13 [00:00<00:00, 42.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 - Train Loss: 13.2822, Val Loss: 4.9356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 49/49 [00:05<00:00,  8.77it/s]\n",
            "Validation Epoch 4/10: 100%|██████████| 13/13 [00:00<00:00, 38.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 4/10 - Train Loss: 8.1245, Val Loss: 4.5425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 49/49 [00:06<00:00,  7.23it/s]\n",
            "Validation Epoch 5/10: 100%|██████████| 13/13 [00:00<00:00, 26.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 5/10 - Train Loss: 3.0273, Val Loss: 6.3904\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3827\n",
            "Precision: 0.1478\n",
            "Recall: 0.3827\n",
            "F1 Score: 0.2131\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.41      1.00      0.58       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.37      0.99      0.54       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.66      0.37      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3377\n",
            "Precision: 0.1313\n",
            "Recall: 0.3377\n",
            "F1 Score: 0.1888\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.85      0.51        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.32      0.89      0.48        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.23      0.58      0.33       385\n",
            "weighted avg       0.13      0.34      0.19       385\n",
            "\n",
            "\n",
            "Basic Model:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:08<00:00,  5.99it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 25.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 13.1761, Val Loss: 4.6793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:07<00:00,  6.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:00<00:00, 42.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/10 - Train Loss: 3.5978, Val Loss: 9.8923\n",
            "No improvement! Early stopping.\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3795\n",
            "Precision: 0.1477\n",
            "Recall: 0.3795\n",
            "F1 Score: 0.2122\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      1.00      0.50       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.41      0.98      0.58       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.25      0.66      0.36      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3299\n",
            "Precision: 0.1302\n",
            "Recall: 0.3299\n",
            "F1 Score: 0.1859\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      0.88      0.43        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.37      0.84      0.51        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.22      0.57      0.31       385\n",
            "weighted avg       0.13      0.33      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.32987012987012987,\n",
              " 0.1302270183188404,\n",
              " 0.32987012987012987,\n",
              " 0.18587855191628777)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing gradient explosion, tweaking clip."
      ],
      "metadata": {
        "id": "CZoXToL9jmNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=10\n",
        "clip=0.75\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i0A24xu7_E7",
        "outputId": "b1c7c82f-f938-46e8-e2ff-c53a2f437b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:10<00:00,  4.84it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:01<00:00,  6.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/10 - Train Loss: 19.2105, Val Loss: 6.4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:07<00:00,  6.83it/s]\n",
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:00<00:00, 42.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Train Loss: 3.1463, Val Loss: 7.0463\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1502\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2154\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      1.00      0.62       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.36      1.00      0.52       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.27      0.67      0.38      1539\n",
            "weighted avg       0.15      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3403\n",
            "Precision: 0.1331\n",
            "Recall: 0.3403\n",
            "F1 Score: 0.1902\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.38      0.75      0.50        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.33      0.95      0.49        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.23      0.57      0.33       385\n",
            "weighted avg       0.13      0.34      0.19       385\n",
            "\n",
            "\n",
            "Basic Model:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:06<00:00,  7.36it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 26.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 13.0947, Val Loss: 7.9441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:07<00:00,  6.32it/s]\n",
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:00<00:00, 27.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Train Loss: 1.0060, Val Loss: 8.1909\n",
            "No improvement! Early stopping.\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3821\n",
            "Precision: 0.1504\n",
            "Recall: 0.3821\n",
            "F1 Score: 0.2149\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.32      1.00      0.48       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.43      0.99      0.60       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.25      0.66      0.36      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3377\n",
            "Precision: 0.1337\n",
            "Recall: 0.3377\n",
            "F1 Score: 0.1909\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.27      0.83      0.41        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.39      0.90      0.54        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.22      0.57      0.32       385\n",
            "weighted avg       0.13      0.34      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.33766233766233766,\n",
              " 0.13371861471861474,\n",
              " 0.33766233766233766,\n",
              " 0.1909363342622454)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even more clip"
      ],
      "metadata": {
        "id": "OZwfGqd_i_58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=10\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2HCQCAF8M-O",
        "outputId": "188e16a7-bbb8-4d62-b5af-03eccc77f2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:08<00:00,  5.77it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 25.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 13.2127, Val Loss: 8.3327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:08<00:00,  5.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:00<00:00, 24.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Train Loss: 2.3649, Val Loss: 8.9371\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.7675\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2184\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      1.00      0.66       206\n",
            "     neutral       1.00      0.00      0.00       946\n",
            "    positive       0.34      1.00      0.51       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.61      0.66      0.39      1539\n",
            "weighted avg       0.77      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3403\n",
            "Precision: 0.7495\n",
            "Recall: 0.3403\n",
            "F1 Score: 0.1966\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.43      0.75      0.55        52\n",
            "     neutral       1.00      0.00      0.01       236\n",
            "    positive       0.31      0.94      0.47        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.58      0.56      0.34       385\n",
            "weighted avg       0.75      0.34      0.20       385\n",
            "\n",
            "\n",
            "Basic Model:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:04<00:00, 10.11it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 44.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/10 - Train Loss: 7.8733, Val Loss: 7.9728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:05<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:00<00:00, 28.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Train Loss: 0.5901, Val Loss: 9.1853\n",
            "No improvement! Early stopping.\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3853\n",
            "Precision: 0.1490\n",
            "Recall: 0.3853\n",
            "F1 Score: 0.2147\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.36      1.00      0.52       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.40      1.00      0.57       387\n",
            "\n",
            "    accuracy                           0.39      1539\n",
            "   macro avg       0.25      0.67      0.37      1539\n",
            "weighted avg       0.15      0.39      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3455\n",
            "Precision: 0.1340\n",
            "Recall: 0.3455\n",
            "F1 Score: 0.1931\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.31      0.83      0.45        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.37      0.93      0.53        97\n",
            "\n",
            "    accuracy                           0.35       385\n",
            "   macro avg       0.22      0.58      0.32       385\n",
            "weighted avg       0.13      0.35      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.34545454545454546,\n",
              " 0.13403657566922875,\n",
              " 0.34545454545454546,\n",
              " 0.19310207336523127)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling back clip to .75 and decreasing lr"
      ],
      "metadata": {
        "id": "RM5yQTQts5uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run = 5"
      ],
      "metadata": {
        "id": "5_a8SgOBG9rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.001\n",
        "epochs=10\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iuo-4aytZkc",
        "outputId": "18b50b67-9a01-44be-8f85-e3beeacbfab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:08<00:00,  5.76it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 18.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/10 - Train Loss: 21.5043, Val Loss: 6.8370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:17<00:00,  2.82it/s]\n",
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:00<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/10 - Train Loss: 1.0933, Val Loss: 7.7039\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3853\n",
            "Precision: 0.1527\n",
            "Recall: 0.3853\n",
            "F1 Score: 0.2174\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      1.00      0.66       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.35      1.00      0.51       387\n",
            "\n",
            "    accuracy                           0.39      1539\n",
            "   macro avg       0.28      0.67      0.39      1539\n",
            "weighted avg       0.15      0.39      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3299\n",
            "Precision: 0.1297\n",
            "Recall: 0.3299\n",
            "F1 Score: 0.1844\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.69      0.48        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.32      0.94      0.47        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.23      0.54      0.32       385\n",
            "weighted avg       0.13      0.33      0.18       385\n",
            "\n",
            "\n",
            "Basic Model:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:07<00:00,  6.33it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 27.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/10 - Train Loss: 17.9517, Val Loss: 6.7332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:14<00:00,  3.37it/s]\n",
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:01<00:00, 12.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Train Loss: 1.5155, Val Loss: 6.4435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 49/49 [00:10<00:00,  4.49it/s]\n",
            "Validation Epoch 3/10: 100%|██████████| 13/13 [00:00<00:00, 25.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/10 - Train Loss: 0.2453, Val Loss: 7.6664\n",
            "No improvement! Early stopping.\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1482\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2140\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.38      1.00      0.55       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.39      1.00      0.56       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.67      0.37      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3299\n",
            "Precision: 0.1274\n",
            "Recall: 0.3299\n",
            "F1 Score: 0.1838\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.27      0.69      0.39        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.36      0.94      0.52        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.21      0.54      0.30       385\n",
            "weighted avg       0.13      0.33      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.32987012987012987,\n",
              " 0.1273819346488698,\n",
              " 0.32987012987012987,\n",
              " 0.18378003296036083)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further decreasing lr"
      ],
      "metadata": {
        "id": "ONFMqycJnSyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.0001\n",
        "epochs=10\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo1rE96LwP1g",
        "outputId": "e88725f3-a28d-4dc3-e772-85dfe08b8e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:09<00:00,  5.35it/s]\n",
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 26.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/10 - Train Loss: 48.3927, Val Loss: 10.5005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:18<00:00,  2.64it/s]\n",
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:01<00:00,  8.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/10 - Train Loss: 37.7305, Val Loss: 8.6879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 49/49 [00:16<00:00,  2.94it/s]\n",
            "Validation Epoch 3/10: 100%|██████████| 13/13 [00:01<00:00, 12.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/10 - Train Loss: 30.0296, Val Loss: 7.6393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 49/49 [00:16<00:00,  3.06it/s]\n",
            "Validation Epoch 4/10: 100%|██████████| 13/13 [00:00<00:00, 26.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 - Train Loss: 23.6309, Val Loss: 7.2441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 49/49 [00:08<00:00,  5.84it/s]\n",
            "Validation Epoch 5/10: 100%|██████████| 13/13 [00:00<00:00, 18.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 - Train Loss: 14.2078, Val Loss: 5.7282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/10: 100%|██████████| 49/49 [00:16<00:00,  2.92it/s]\n",
            "Validation Epoch 6/10: 100%|██████████| 13/13 [00:01<00:00, 12.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 - Train Loss: 5.3793, Val Loss: 6.7253\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1530\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2174\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      1.00      0.67       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.34      1.00      0.51       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.28      0.67      0.39      1539\n",
            "weighted avg       0.15      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3195\n",
            "Precision: 0.1267\n",
            "Recall: 0.3195\n",
            "F1 Score: 0.1789\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.65      0.48        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.30      0.92      0.46        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.23      0.52      0.31       385\n",
            "weighted avg       0.13      0.32      0.18       385\n",
            "\n",
            "\n",
            "Basic Model:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 49/49 [00:07<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/10: 100%|██████████| 13/13 [00:00<00:00, 27.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 42.3478, Val Loss: 10.2530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 49/49 [00:13<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/10: 100%|██████████| 13/13 [00:01<00:00, 12.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/10 - Train Loss: 31.6065, Val Loss: 8.1686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 49/49 [00:11<00:00,  4.31it/s]\n",
            "Validation Epoch 3/10: 100%|██████████| 13/13 [00:00<00:00, 27.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/10 - Train Loss: 25.0412, Val Loss: 7.1127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 49/49 [00:09<00:00,  5.09it/s]\n",
            "Validation Epoch 4/10: 100%|██████████| 13/13 [00:00<00:00, 14.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 4/10 - Train Loss: 17.3616, Val Loss: 5.9120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 49/49 [00:16<00:00,  2.91it/s]\n",
            "Validation Epoch 5/10: 100%|██████████| 13/13 [00:00<00:00, 13.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 5/10 - Train Loss: 8.2553, Val Loss: 4.9267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/10: 100%|██████████| 49/49 [00:16<00:00,  3.00it/s]\n",
            "Validation Epoch 6/10: 100%|██████████| 13/13 [00:00<00:00, 14.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 - Train Loss: 3.4185, Val Loss: 6.5730\n",
            "No improvement! Early stopping.\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3840\n",
            "Precision: 0.1480\n",
            "Recall: 0.3840\n",
            "F1 Score: 0.2136\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.39      1.00      0.56       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.38      0.99      0.55       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.66      0.37      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3247\n",
            "Precision: 0.1248\n",
            "Recall: 0.3247\n",
            "F1 Score: 0.1801\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      0.65      0.39        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.34      0.94      0.50        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.21      0.53      0.30       385\n",
            "weighted avg       0.12      0.32      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3246753246753247,\n",
              " 0.1247978605416622,\n",
              " 0.3246753246753247,\n",
              " 0.18011000896050458)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying different hidden sizes, 64 and 128"
      ],
      "metadata": {
        "id": "MbYqlpIUnIFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 64 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20 # Increasing epochs just in case\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 128 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20 # Increasing epochs just in case\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPDUSMsZsv1W",
        "outputId": "e83e70ee-7288-4e0c-8a37-e5a759b80d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:16<00:00,  2.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00,  9.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 8.2694, Val Loss: 10.8903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:19<00:00,  2.51it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 21.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 0.0911, Val Loss: 11.7784\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1546\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2185\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.52      1.00      0.68       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.34      1.00      0.51       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.29      0.67      0.40      1539\n",
            "weighted avg       0.15      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3299\n",
            "Precision: 0.1319\n",
            "Recall: 0.3299\n",
            "F1 Score: 0.1862\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.41      0.75      0.53        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.30      0.91      0.45        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.24      0.55      0.33       385\n",
            "weighted avg       0.13      0.33      0.19       385\n",
            "\n",
            "\n",
            "Basic Model:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:09<00:00,  5.21it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 23.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 7.6358, Val Loss: 10.3715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:12<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 15.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 1.2271, Val Loss: 10.9748\n",
            "No improvement! Early stopping.\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3827\n",
            "Precision: 0.1565\n",
            "Recall: 0.3827\n",
            "F1 Score: 0.2198\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      1.00      0.44       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.47      0.99      0.64       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.25      0.66      0.36      1539\n",
            "weighted avg       0.16      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3221\n",
            "Precision: 0.1319\n",
            "Recall: 0.3221\n",
            "F1 Score: 0.1852\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.24      0.85      0.38        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.39      0.82      0.53        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.21      0.56      0.30       385\n",
            "weighted avg       0.13      0.32      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:11<00:00,  4.11it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00,  9.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 6.5909, Val Loss: 10.2373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:19<00:00,  2.46it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 17.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 2.4339, Val Loss: 7.5890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:15<00:00,  3.26it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00,  9.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 0.2911, Val Loss: 13.6894\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3834\n",
            "Precision: 0.1495\n",
            "Recall: 0.3834\n",
            "F1 Score: 0.2146\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      1.00      0.50       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.42      0.99      0.59       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.25      0.66      0.36      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3351\n",
            "Precision: 0.1311\n",
            "Recall: 0.3351\n",
            "F1 Score: 0.1881\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      0.83      0.42        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.37      0.89      0.52        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.22      0.57      0.31       385\n",
            "weighted avg       0.13      0.34      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:12<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 17.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 5.5019, Val Loss: 11.4935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:17<00:00,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00,  9.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 1.6774, Val Loss: 12.2560\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1482\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2140\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.38      1.00      0.56       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.38      1.00      0.56       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.67      0.37      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3377\n",
            "Precision: 0.1316\n",
            "Recall: 0.3377\n",
            "F1 Score: 0.1893\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.29      0.81      0.42        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.37      0.91      0.52        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.22      0.57      0.32       385\n",
            "weighted avg       0.13      0.34      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.33766233766233766,\n",
              " 0.13162177995100563,\n",
              " 0.33766233766233766,\n",
              " 0.18927306459773993)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "81 remains the best but continuing to see if this is improved by a larger hidden size"
      ],
      "metadata": {
        "id": "WSWmxgivpBy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 256 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.0001\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 512 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.0001\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFcpqjQitUhw",
        "outputId": "da0e88c5-0ba9-4c21-8aba-8e779d2afd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:41<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00,  7.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 36.0615, Val Loss: 7.5875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:33<00:00,  1.47it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 16.7871, Val Loss: 5.6825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:33<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 2.1425, Val Loss: 9.3761\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3827\n",
            "Precision: 0.1520\n",
            "Recall: 0.3827\n",
            "F1 Score: 0.2161\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      0.98      0.65       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.34      1.00      0.51       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.28      0.66      0.39      1539\n",
            "weighted avg       0.15      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3195\n",
            "Precision: 0.1266\n",
            "Recall: 0.3195\n",
            "F1 Score: 0.1783\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.62      0.46        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.31      0.94      0.46        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.22      0.52      0.31       385\n",
            "weighted avg       0.13      0.32      0.18       385\n",
            "\n",
            "\n",
            "Basic Model:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:43<00:00,  1.13it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 38.5844, Val Loss: 7.9565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:15<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00,  8.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 15.7929, Val Loss: 6.8537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:22<00:00,  2.23it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 2.9820, Val Loss: 6.7135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:17<00:00,  2.82it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:01<00:00,  7.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 1.5175, Val Loss: 10.3093\n",
            "No improvement! Early stopping.\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3840\n",
            "Precision: 0.1490\n",
            "Recall: 0.3840\n",
            "F1 Score: 0.2144\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.34      1.00      0.51       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.41      0.99      0.58       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.25      0.66      0.36      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3065\n",
            "Precision: 0.1214\n",
            "Recall: 0.3065\n",
            "F1 Score: 0.1735\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.23      0.69      0.35        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.36      0.85      0.50        97\n",
            "\n",
            "    accuracy                           0.31       385\n",
            "   macro avg       0.20      0.51      0.28       385\n",
            "weighted avg       0.12      0.31      0.17       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [01:00<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:05<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 29.7472, Val Loss: 6.7034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [01:05<00:00,  1.34s/it]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 5.3746, Val Loss: 6.9416\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3834\n",
            "Precision: 0.1479\n",
            "Recall: 0.3834\n",
            "F1 Score: 0.2134\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.40      1.00      0.57       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.37      0.99      0.54       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.66      0.37      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3039\n",
            "Precision: 0.1172\n",
            "Recall: 0.3039\n",
            "F1 Score: 0.1689\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.29      0.65      0.40        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.31      0.86      0.46        97\n",
            "\n",
            "    accuracy                           0.30       385\n",
            "   macro avg       0.20      0.50      0.29       385\n",
            "weighted avg       0.12      0.30      0.17       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:55<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:05<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 31.1715, Val Loss: 6.3967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [01:08<00:00,  1.40s/it]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:05<00:00,  2.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 4.8159, Val Loss: 12.9506\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3827\n",
            "Precision: 0.1476\n",
            "Recall: 0.3827\n",
            "F1 Score: 0.2130\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.99      0.54       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.39      0.99      0.56       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.25      0.66      0.37      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3065\n",
            "Precision: 0.1187\n",
            "Recall: 0.3065\n",
            "F1 Score: 0.1712\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.26      0.67      0.37        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.33      0.86      0.48        97\n",
            "\n",
            "    accuracy                           0.31       385\n",
            "   macro avg       0.20      0.51      0.28       385\n",
            "weighted avg       0.12      0.31      0.17       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3064935064935065,\n",
              " 0.11874204227145402,\n",
              " 0.3064935064935065,\n",
              " 0.17116694644673983)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying smaller"
      ],
      "metadata": {
        "id": "QVKCDJZhOl75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 32 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.0001\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScwnMw8nOhfd",
        "outputId": "41415ced-1a0e-4def-fd9c-e3ff066620e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:05<00:00,  8.58it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 69.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 47.7896, Val Loss: 11.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:07<00:00,  6.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 25.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 41.0623, Val Loss: 9.8857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:12<00:00,  3.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 22.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 36.5482, Val Loss: 9.7064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:11<00:00,  4.11it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:00<00:00, 23.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 33.5614, Val Loss: 9.2576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 49/49 [00:12<00:00,  3.99it/s]\n",
            "Validation Epoch 5/20: 100%|██████████| 13/13 [00:00<00:00, 40.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Train Loss: 30.9195, Val Loss: 8.8047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/20: 100%|██████████| 49/49 [00:06<00:00,  7.37it/s]\n",
            "Validation Epoch 6/20: 100%|██████████| 13/13 [00:00<00:00, 20.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 6/20 - Train Loss: 28.5755, Val Loss: 7.8417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7/20: 100%|██████████| 49/49 [00:11<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 7/20: 100%|██████████| 13/13 [00:00<00:00, 26.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 7/20 - Train Loss: 24.8712, Val Loss: 7.2356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8/20: 100%|██████████| 49/49 [00:07<00:00,  6.15it/s]\n",
            "Validation Epoch 8/20: 100%|██████████| 13/13 [00:00<00:00, 66.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 8/20 - Train Loss: 21.4773, Val Loss: 6.8237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9/20: 100%|██████████| 49/49 [00:03<00:00, 13.15it/s]\n",
            "Validation Epoch 9/20: 100%|██████████| 13/13 [00:00<00:00, 58.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 - Train Loss: 17.9423, Val Loss: 7.1280\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3710\n",
            "Precision: 0.1761\n",
            "Recall: 0.3710\n",
            "F1 Score: 0.2253\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.90      0.82       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.30      1.00      0.46       387\n",
            "\n",
            "    accuracy                           0.37      1539\n",
            "   macro avg       0.35      0.63      0.43      1539\n",
            "weighted avg       0.18      0.37      0.23      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3039\n",
            "Precision: 0.1497\n",
            "Recall: 0.3039\n",
            "F1 Score: 0.1777\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.61      0.48      0.54        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.27      0.95      0.42        97\n",
            "\n",
            "    accuracy                           0.30       385\n",
            "   macro avg       0.29      0.48      0.32       385\n",
            "weighted avg       0.15      0.30      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:03<00:00, 14.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 71.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 46.4158, Val Loss: 10.9268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:04<00:00, 11.09it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 46.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 40.0825, Val Loss: 9.4747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:05<00:00,  9.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 49.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 33.9909, Val Loss: 9.3166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:05<00:00,  9.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:00<00:00, 45.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 4/20 - Train Loss: 30.2000, Val Loss: 8.0936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 49/49 [00:05<00:00,  9.21it/s]\n",
            "Validation Epoch 5/20: 100%|██████████| 13/13 [00:00<00:00, 47.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Train Loss: 26.5938, Val Loss: 8.4997\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2930\n",
            "Precision: 0.1778\n",
            "Recall: 0.2930\n",
            "F1 Score: 0.1658\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.83      0.31      0.45       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.26      1.00      0.42       387\n",
            "\n",
            "    accuracy                           0.29      1539\n",
            "   macro avg       0.37      0.44      0.29      1539\n",
            "weighted avg       0.18      0.29      0.17      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2623\n",
            "Precision: 0.1486\n",
            "Recall: 0.2623\n",
            "F1 Score: 0.1246\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.62      0.10      0.17        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.25      0.99      0.41        97\n",
            "\n",
            "    accuracy                           0.26       385\n",
            "   macro avg       0.29      0.36      0.19       385\n",
            "weighted avg       0.15      0.26      0.12       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2623376623376623,\n",
              " 0.14857211753763477,\n",
              " 0.2623376623376623,\n",
              " 0.12456572962902077)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for giggles 1024 and 16"
      ],
      "metadata": {
        "id": "EX2m6MpMPN1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 1024  # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.0001\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 16 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.0001\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQMVP4MkPNlw",
        "outputId": "999424b4-0af4-4630-dae4-a526660c6f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Training Epoch 1/20: 100%|██████████| 49/49 [04:52<00:00,  5.97s/it]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:20<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 22.1783, Val Loss: 11.8317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [04:26<00:00,  5.44s/it]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:16<00:00,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 2.7747, Val Loss: 14.2394\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3814\n",
            "Precision: 0.1526\n",
            "Recall: 0.3814\n",
            "F1 Score: 0.2161\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.51      0.98      0.67       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.34      0.99      0.50       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.28      0.66      0.39      1539\n",
            "weighted avg       0.15      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3247\n",
            "Precision: 0.1275\n",
            "Recall: 0.3247\n",
            "F1 Score: 0.1804\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.36      0.62      0.45        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.32      0.96      0.47        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.22      0.52      0.31       385\n",
            "weighted avg       0.13      0.32      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [03:58<00:00,  4.87s/it]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:21<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 24.2684, Val Loss: 11.8751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [03:40<00:00,  4.51s/it]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:19<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 5.1148, Val Loss: 14.0110\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3730\n",
            "Precision: 0.1495\n",
            "Recall: 0.3730\n",
            "F1 Score: 0.2111\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      0.91      0.64       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.33      1.00      0.50       387\n",
            "\n",
            "    accuracy                           0.37      1539\n",
            "   macro avg       0.27      0.64      0.38      1539\n",
            "weighted avg       0.15      0.37      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3169\n",
            "Precision: 0.1251\n",
            "Recall: 0.3169\n",
            "F1 Score: 0.1756\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.35      0.56      0.43        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.31      0.96      0.47        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.22      0.51      0.30       385\n",
            "weighted avg       0.13      0.32      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:05<00:00,  9.19it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 47.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 54.0508, Val Loss: 12.9670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:05<00:00,  9.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 47.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 50.3348, Val Loss: 12.3683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:05<00:00,  8.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 38.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 47.8911, Val Loss: 11.8474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:05<00:00,  8.19it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:00<00:00, 52.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 46.1826, Val Loss: 12.2163\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2684\n",
            "Precision: 0.6020\n",
            "Recall: 0.2684\n",
            "F1 Score: 0.1338\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.12      0.20       206\n",
            "     neutral       0.75      0.00      0.01       946\n",
            "    positive       0.26      0.99      0.41       387\n",
            "\n",
            "    accuracy                           0.27      1539\n",
            "   macro avg       0.53      0.37      0.21      1539\n",
            "weighted avg       0.60      0.27      0.13      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2649\n",
            "Precision: 0.7672\n",
            "Recall: 0.2649\n",
            "F1 Score: 0.1310\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.67      0.08      0.14        52\n",
            "     neutral       1.00      0.01      0.02       236\n",
            "    positive       0.25      0.99      0.41        97\n",
            "\n",
            "    accuracy                           0.26       385\n",
            "   macro avg       0.64      0.36      0.19       385\n",
            "weighted avg       0.77      0.26      0.13       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:05<00:00,  9.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 96.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 45.7577, Val Loss: 12.0677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:03<00:00, 16.33it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 95.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 41.7105, Val Loss: 9.8955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:03<00:00, 16.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 89.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 36.6168, Val Loss: 9.9874\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3112\n",
            "Precision: 0.1251\n",
            "Recall: 0.3112\n",
            "F1 Score: 0.1718\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.38      0.50      0.43       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.30      0.97      0.46       387\n",
            "\n",
            "    accuracy                           0.31      1539\n",
            "   macro avg       0.22      0.49      0.29      1539\n",
            "weighted avg       0.13      0.31      0.17      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2571\n",
            "Precision: 0.0951\n",
            "Recall: 0.2571\n",
            "F1 Score: 0.1304\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.21      0.19      0.20        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.26      0.92      0.41        97\n",
            "\n",
            "    accuracy                           0.26       385\n",
            "   macro avg       0.16      0.37      0.20       385\n",
            "weighted avg       0.10      0.26      0.13       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2571428571428571,\n",
              " 0.0950785711833175,\n",
              " 0.2571428571428571,\n",
              " 0.13038182994295847)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying the earlier best params. Tweaking gru layers. This also introduces dropout."
      ],
      "metadata": {
        "id": "yoF7cbTgqGHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 2 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfLff-OerMTR",
        "outputId": "56922abd-5f35-4828-8b56-bb861f301ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:31<00:00,  1.56it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00, 12.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 17.8893, Val Loss: 8.4266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 2.1139, Val Loss: 9.7842\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1588\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2213\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      1.00      0.73       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.33      1.00      0.49       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.30      0.67      0.41      1539\n",
            "weighted avg       0.16      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3273\n",
            "Precision: 0.1355\n",
            "Recall: 0.3273\n",
            "F1 Score: 0.1870\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.46      0.71      0.56        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.29      0.92      0.44        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.25      0.54      0.33       385\n",
            "weighted avg       0.14      0.33      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:26<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00, 12.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 6.9517, Val Loss: 11.1758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:21<00:00,  2.23it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00, 12.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 1.9291, Val Loss: 17.0211\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1523\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2169\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      1.00      0.65       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.35      1.00      0.51       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.28      0.67      0.39      1539\n",
            "weighted avg       0.15      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3247\n",
            "Precision: 0.1266\n",
            "Recall: 0.3247\n",
            "F1 Score: 0.1805\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.34      0.65      0.45        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.32      0.94      0.48        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.22      0.53      0.31       385\n",
            "weighted avg       0.13      0.32      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3246753246753247,\n",
              " 0.12655122655122655,\n",
              " 0.3246753246753247,\n",
              " 0.18054860602886816)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying 2 layers with variations on dropout"
      ],
      "metadata": {
        "id": "qMkBb68wsCjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 2 # 1 to start\n",
        "dropout = 0.25\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 2 # 1 to start\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 2 # 1 to start\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aboh77A7sCLz",
        "outputId": "bbab733f-07d7-4148-a8db-fa5d54e25b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:28<00:00,  1.72it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:02<00:00,  6.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 9.0741, Val Loss: 12.3080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:27<00:00,  1.79it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:02<00:00,  5.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 0.3536, Val Loss: 13.8619\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3814\n",
            "Precision: 0.1682\n",
            "Recall: 0.3814\n",
            "F1 Score: 0.2280\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.25      1.00      0.40       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.54      0.98      0.69       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.66      0.36      1539\n",
            "weighted avg       0.17      0.38      0.23      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3247\n",
            "Precision: 0.1537\n",
            "Recall: 0.3247\n",
            "F1 Score: 0.2005\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.21      0.90      0.33        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.50      0.80      0.62        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.24      0.57      0.32       385\n",
            "weighted avg       0.15      0.32      0.20       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:17<00:00,  2.87it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00, 11.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 9.3918, Val Loss: 9.6339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:18<00:00,  2.69it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00, 10.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 1.5090, Val Loss: 18.8376\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3840\n",
            "Precision: 0.1654\n",
            "Recall: 0.3840\n",
            "F1 Score: 0.2251\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.64      0.99      0.78       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.32      1.00      0.48       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.32      0.66      0.42      1539\n",
            "weighted avg       0.17      0.38      0.23      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3143\n",
            "Precision: 0.1340\n",
            "Recall: 0.3143\n",
            "F1 Score: 0.1792\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.46      0.58      0.51        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.28      0.94      0.44        97\n",
            "\n",
            "    accuracy                           0.31       385\n",
            "   macro avg       0.25      0.51      0.32       385\n",
            "weighted avg       0.13      0.31      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:13<00:00,  3.54it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 19.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 15.8742, Val Loss: 14.4705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:18<00:00,  2.71it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00, 11.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 4.3737, Val Loss: 19.8359\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3814\n",
            "Precision: 0.1695\n",
            "Recall: 0.3814\n",
            "F1 Score: 0.2265\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.68      0.97      0.80       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.31      1.00      0.47       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.33      0.66      0.43      1539\n",
            "weighted avg       0.17      0.38      0.23      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3273\n",
            "Precision: 0.1561\n",
            "Recall: 0.3273\n",
            "F1 Score: 0.1947\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.63      0.63      0.63        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.28      0.96      0.43        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.30      0.53      0.36       385\n",
            "weighted avg       0.16      0.33      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:12<00:00,  4.07it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00, 12.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 6.6092, Val Loss: 10.4607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:16<00:00,  2.97it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 20.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 1.6112, Val Loss: 10.9185\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1606\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2224\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.59      1.00      0.74       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.32      1.00      0.49       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.30      0.67      0.41      1539\n",
            "weighted avg       0.16      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3377\n",
            "Precision: 0.1494\n",
            "Recall: 0.3377\n",
            "F1 Score: 0.1992\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.79      0.67        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.28      0.92      0.43        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.29      0.57      0.37       385\n",
            "weighted avg       0.15      0.34      0.20       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:14<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 16.4302, Val Loss: 14.1702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:15<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 18.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 1.6925, Val Loss: 14.8668\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3853\n",
            "Precision: 0.1631\n",
            "Recall: 0.3853\n",
            "F1 Score: 0.2243\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.61      1.00      0.76       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.32      1.00      0.49       387\n",
            "\n",
            "    accuracy                           0.39      1539\n",
            "   macro avg       0.31      0.67      0.42      1539\n",
            "weighted avg       0.16      0.39      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3325\n",
            "Precision: 0.1421\n",
            "Recall: 0.3325\n",
            "F1 Score: 0.1932\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.52      0.77      0.62        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.29      0.91      0.43        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.27      0.56      0.35       385\n",
            "weighted avg       0.14      0.33      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:15<00:00,  3.09it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 6.4566, Val Loss: 18.0799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:13<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 20.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 2.7438, Val Loss: 15.1160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:15<00:00,  3.17it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00, 12.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 0.5194, Val Loss: 18.6197\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3853\n",
            "Precision: 0.1503\n",
            "Recall: 0.3853\n",
            "F1 Score: 0.2157\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      1.00      0.62       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.36      1.00      0.53       387\n",
            "\n",
            "    accuracy                           0.39      1539\n",
            "   macro avg       0.27      0.67      0.38      1539\n",
            "weighted avg       0.15      0.39      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3221\n",
            "Precision: 0.1259\n",
            "Recall: 0.3221\n",
            "F1 Score: 0.1801\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.36      0.73      0.48        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.31      0.89      0.46        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.22      0.54      0.31       385\n",
            "weighted avg       0.13      0.32      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3220779220779221,\n",
              " 0.12590775055644282,\n",
              " 0.3220779220779221,\n",
              " 0.1801195131912113)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying different values for encoder decoder length ie 1 or 2 for encoder or decoder."
      ],
      "metadata": {
        "id": "vd07QslxsU0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "#gru_layers = 2 # 1 to start\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers=2, dropout=dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers=1, dropout=dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "#gru_layers = 2 # 1 to start\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers=1, dropout=dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers=2, dropout=dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOdmo2mqsdRF",
        "outputId": "eedd289e-d113-4516-ae81-cbd09015f2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:36<00:00,  1.34it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:02<00:00,  6.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 6.3307, Val Loss: 13.0189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:24<00:00,  2.02it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:02<00:00,  5.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 0.9260, Val Loss: 14.4461\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3853\n",
            "Precision: 0.1512\n",
            "Recall: 0.3853\n",
            "F1 Score: 0.2163\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.47      1.00      0.64       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.35      1.00      0.52       387\n",
            "\n",
            "    accuracy                           0.39      1539\n",
            "   macro avg       0.27      0.67      0.39      1539\n",
            "weighted avg       0.15      0.39      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3273\n",
            "Precision: 0.1284\n",
            "Recall: 0.3273\n",
            "F1 Score: 0.1828\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.36      0.69      0.48        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.31      0.93      0.47        97\n",
            "\n",
            "    accuracy                           0.33       385\n",
            "   macro avg       0.23      0.54      0.32       385\n",
            "weighted avg       0.13      0.33      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:08<00:00,  6.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 37.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 15.6699, Val Loss: 13.6562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:06<00:00,  7.88it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 37.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 2.5242, Val Loss: 17.2715\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1485\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2142\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.41      1.00      0.58       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.37      1.00      0.54       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.67      0.37      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3169\n",
            "Precision: 0.1235\n",
            "Recall: 0.3169\n",
            "F1 Score: 0.1771\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.34      0.73      0.47        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.31      0.87      0.45        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.22      0.53      0.31       385\n",
            "weighted avg       0.12      0.32      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3168831168831169,\n",
              " 0.12347799501084174,\n",
              " 0.3168831168831169,\n",
              " 0.17706474564907254)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still no real improvement. Try more layers 3, 4, and 5."
      ],
      "metadata": {
        "id": "N21NawtUsU7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 3\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 4\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 5\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "\n",
        "model_run += 1\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtHdWoQ9vNYA",
        "outputId": "efdb0d44-25c4-4fff-bbd5-2c5c261d1233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:55<00:00,  1.13s/it]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:03<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 36.0956, Val Loss: 8.6523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:36<00:00,  1.36it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00,  7.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 36.7786, Val Loss: 7.7489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:22<00:00,  2.18it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00,  7.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 35.8293, Val Loss: 7.7280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:24<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:01<00:00, 12.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 4/20 - Train Loss: 34.5132, Val Loss: 8.5699\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.1371\n",
            "Precision: 0.0694\n",
            "Recall: 0.1371\n",
            "F1 Score: 0.0433\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.13      0.98      0.24       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.20      0.03      0.05       387\n",
            "\n",
            "    accuracy                           0.14      1539\n",
            "   macro avg       0.11      0.33      0.09      1539\n",
            "weighted avg       0.07      0.14      0.04      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.1377\n",
            "Precision: 0.0763\n",
            "Recall: 0.1377\n",
            "F1 Score: 0.0456\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.13      0.96      0.24        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.23      0.03      0.05        97\n",
            "\n",
            "    accuracy                           0.14       385\n",
            "   macro avg       0.12      0.33      0.10       385\n",
            "weighted avg       0.08      0.14      0.05       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:21<00:00,  2.24it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 13.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 7.0667, Val Loss: 11.8637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:23<00:00,  2.05it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 13.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 0.1750, Val Loss: 20.6192\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3853\n",
            "Precision: 0.1614\n",
            "Recall: 0.3853\n",
            "F1 Score: 0.2232\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.60      1.00      0.75       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.32      1.00      0.49       387\n",
            "\n",
            "    accuracy                           0.39      1539\n",
            "   macro avg       0.31      0.67      0.41      1539\n",
            "weighted avg       0.16      0.39      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3169\n",
            "Precision: 0.1322\n",
            "Recall: 0.3169\n",
            "F1 Score: 0.1810\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      0.65      0.53        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.28      0.91      0.43        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.24      0.52      0.32       385\n",
            "weighted avg       0.13      0.32      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:31<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 36.5346, Val Loss: 8.8260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:59<00:00,  1.22s/it]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:02<00:00,  5.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 33.2827, Val Loss: 7.9032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:34<00:00,  1.43it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:02<00:00,  5.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 35.1875, Val Loss: 7.8970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:32<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:01<00:00,  8.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 4/20 - Train Loss: 34.9865, Val Loss: 7.7771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 49/49 [00:32<00:00,  1.49it/s]\n",
            "Validation Epoch 5/20: 100%|██████████| 13/13 [00:02<00:00,  5.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Train Loss: 33.6536, Val Loss: 9.2886\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2515\n",
            "Precision: 0.0632\n",
            "Recall: 0.2515\n",
            "F1 Score: 0.1011\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.25      1.00      0.40       387\n",
            "\n",
            "    accuracy                           0.25      1539\n",
            "   macro avg       0.08      0.33      0.13      1539\n",
            "weighted avg       0.06      0.25      0.10      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2519\n",
            "Precision: 0.0635\n",
            "Recall: 0.2519\n",
            "F1 Score: 0.1014\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.25      1.00      0.40        97\n",
            "\n",
            "    accuracy                           0.25       385\n",
            "   macro avg       0.08      0.33      0.13       385\n",
            "weighted avg       0.06      0.25      0.10       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:32<00:00,  1.50it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:04<00:00,  2.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 7.3407, Val Loss: 14.9110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:51<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:04<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 2.6022, Val Loss: 20.4313\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3827\n",
            "Precision: 0.1712\n",
            "Recall: 0.3827\n",
            "F1 Score: 0.2280\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.70      0.98      0.81       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.31      1.00      0.47       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.34      0.66      0.43      1539\n",
            "weighted avg       0.17      0.38      0.23      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3195\n",
            "Precision: 0.1441\n",
            "Recall: 0.3195\n",
            "F1 Score: 0.1871\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.55      0.63      0.59        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.28      0.93      0.43        97\n",
            "\n",
            "    accuracy                           0.32       385\n",
            "   macro avg       0.28      0.52      0.34       385\n",
            "weighted avg       0.14      0.32      0.19       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [01:05<00:00,  1.33s/it]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:02<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 37.8154, Val Loss: 7.7955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:47<00:00,  1.03it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:02<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 36.3596, Val Loss: 7.7323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:47<00:00,  1.03it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00,  7.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 35.9107, Val Loss: 8.5541\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2515\n",
            "Precision: 0.0632\n",
            "Recall: 0.2515\n",
            "F1 Score: 0.1011\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.25      1.00      0.40       387\n",
            "\n",
            "    accuracy                           0.25      1539\n",
            "   macro avg       0.08      0.33      0.13      1539\n",
            "weighted avg       0.06      0.25      0.10      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2519\n",
            "Precision: 0.0635\n",
            "Recall: 0.2519\n",
            "F1 Score: 0.1014\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.25      1.00      0.40        97\n",
            "\n",
            "    accuracy                           0.25       385\n",
            "   macro avg       0.08      0.33      0.13       385\n",
            "weighted avg       0.06      0.25      0.10       385\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:38<00:00,  1.28it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:02<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 8.1548, Val Loss: 28.9997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:37<00:00,  1.31it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00,  7.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 3.3715, Val Loss: 15.3384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:35<00:00,  1.39it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00,  7.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 4.9358, Val Loss: 15.0878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:42<00:00,  1.16it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:02<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 4/20 - Train Loss: 1.0929, Val Loss: 21.2762\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3847\n",
            "Precision: 0.1573\n",
            "Recall: 0.3847\n",
            "F1 Score: 0.2203\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.55      1.00      0.71       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.33      1.00      0.50       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.29      0.67      0.40      1539\n",
            "weighted avg       0.16      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3091\n",
            "Precision: 0.1285\n",
            "Recall: 0.3091\n",
            "F1 Score: 0.1750\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.42      0.58      0.49        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.28      0.92      0.43        97\n",
            "\n",
            "    accuracy                           0.31       385\n",
            "   macro avg       0.24      0.50      0.31       385\n",
            "weighted avg       0.13      0.31      0.18       385\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3090909090909091,\n",
              " 0.12848171833549066,\n",
              " 0.3090909090909091,\n",
              " 0.17500152212632183)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see clearly that more layers are not helping and sometimes strongly hurting. The GRU2GRU model becomes worthless as it starts predicting everything as either positive or negative. Unfortunately it seems our model is simply unable to predict neutral values outside of the occasional random few that aren't sorted into negative or positive. We also run into the question of what thebest model is when the performance is so off.\n",
        "\n",
        "We will be loading the best GRU2GRU and Basic model and evaluating the test set.\n",
        "\n",
        "From there we will use the best parameters and try different word dim vectors and different agreements"
      ],
      "metadata": {
        "id": "-fYScSk9voZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model Run 28:\\n')\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 2 # 1 to start\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "best_g2g_model = GRU2GRU(encoder, decoder)\n",
        "best_g2g_model.load_state_dict(torch.load('./best_model_run28.pt'))\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(best_g2g_model, test_loader, device, class_names)\n",
        "\n",
        "\n",
        "print('\\nGRU2GRU Model Run 30:\\n')\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 2 # 1 to start\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "best_g2g_model = GRU2GRU(encoder, decoder)\n",
        "best_g2g_model.load_state_dict(torch.load('./best_model_run30.pt'))\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(best_g2g_model, test_loader, device, class_names)\n",
        "\n",
        "\n",
        "print('\\nBasic Model Run 5:\\n')\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=10\n",
        "clip=0.9\n",
        "\n",
        "best_simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "best_simple_model.load_state_dict(torch.load('./best_model_run5.pt'))\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(best_simple_model, test_loader, device, class_names)\n",
        "\n",
        "\n",
        "print('\\nBasic Model Run 29:\\n')\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 2 # 1 to start\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "best_simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "best_simple_model.load_state_dict(torch.load('./best_model_run29.pt'))\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(best_simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrYQWJlykPwY",
        "outputId": "73deb203-8026-4305-ade2-7d4ce91ef40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GRU2GRU Model Run 28:\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3059\n",
            "Precision: 0.1498\n",
            "Recall: 0.3059\n",
            "F1 Score: 0.1924\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.18      0.84      0.30        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.50      0.77      0.60        86\n",
            "\n",
            "    accuracy                           0.31       340\n",
            "   macro avg       0.23      0.54      0.30       340\n",
            "weighted avg       0.15      0.31      0.19       340\n",
            "\n",
            "\n",
            "GRU2GRU Model Run 30:\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3088\n",
            "Precision: 0.1477\n",
            "Recall: 0.3088\n",
            "F1 Score: 0.1809\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.60      0.53      0.56        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.27      0.94      0.42        86\n",
            "\n",
            "    accuracy                           0.31       340\n",
            "   macro avg       0.29      0.49      0.33       340\n",
            "weighted avg       0.15      0.31      0.18       340\n",
            "\n",
            "\n",
            "Basic Model Run 5:\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3471\n",
            "Precision: 0.1365\n",
            "Recall: 0.3471\n",
            "F1 Score: 0.1944\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.41      0.80      0.54        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.33      0.95      0.49        86\n",
            "\n",
            "    accuracy                           0.35       340\n",
            "   macro avg       0.24      0.58      0.34       340\n",
            "weighted avg       0.14      0.35      0.19       340\n",
            "\n",
            "\n",
            "Basic Model Run 29:\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3147\n",
            "Precision: 0.1221\n",
            "Recall: 0.3147\n",
            "F1 Score: 0.1750\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.34      0.69      0.45        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.31      0.88      0.46        86\n",
            "\n",
            "    accuracy                           0.31       340\n",
            "   macro avg       0.21      0.52      0.30       340\n",
            "weighted avg       0.12      0.31      0.18       340\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.31470588235294117,\n",
              " 0.12211141819981851,\n",
              " 0.31470588235294117,\n",
              " 0.17500790604278776)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nGRU2GRU Model Run 0:\\n')\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=10\n",
        "clip=0.25\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "best_g2g_model = GRU2GRU(encoder, decoder)\n",
        "best_g2g_model.load_state_dict(torch.load('./best_model_run0.pt'))\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(best_g2g_model, test_loader, device, class_names)\n",
        "\n",
        "\n",
        "print('\\nGRU2GRU Model Run 4:\\n')\n",
        "\n",
        "hidden_size = 81 # max seq length.\n",
        "gru_layers = 1 # 1 to start\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=10\n",
        "clip=0.9\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "best_g2g_model = GRU2GRU(encoder, decoder)\n",
        "best_g2g_model.load_state_dict(torch.load('./best_model_run4.pt'))\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(best_g2g_model, test_loader, device, class_names)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL8I4mtrtYJM",
        "outputId": "df5106ac-5c47-4911-e653-54aed1432fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GRU2GRU Model Run 0:\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3324\n",
            "Precision: 0.1279\n",
            "Recall: 0.3324\n",
            "F1 Score: 0.1845\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      0.76      0.46        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.33      0.92      0.49        86\n",
            "\n",
            "    accuracy                           0.33       340\n",
            "   macro avg       0.22      0.56      0.32       340\n",
            "weighted avg       0.13      0.33      0.18       340\n",
            "\n",
            "\n",
            "GRU2GRU Model Run 4:\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3412\n",
            "Precision: 0.1337\n",
            "Recall: 0.3412\n",
            "F1 Score: 0.1916\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      0.84      0.42        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.38      0.91      0.54        86\n",
            "\n",
            "    accuracy                           0.34       340\n",
            "   macro avg       0.22      0.58      0.32       340\n",
            "weighted avg       0.13      0.34      0.19       340\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3411764705882353,\n",
              " 0.13369377162629756,\n",
              " 0.3411764705882353,\n",
              " 0.19163851938184304)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We elect to use model 30 and model 5 for the GRU2GRU and basic model respectively as the \"best\" models for the full run through.\n",
        "\n",
        "While running the below models we observed a constant issue where every GRU2GRU model would put all the data points as positive. We suspected that was because we had a defined random seed in the test train splits, but even after removing the random_state variable we consistently saw this behavior. We are baffled by it and have no explanation for it."
      ],
      "metadata": {
        "id": "r-Y4xW_5t-HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "50 Dim Word Vectors all Agreements"
      ],
      "metadata": {
        "id": "upy82zrP0dCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.50d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_AllAgree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=50)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '50dim_AllAgree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '50dim_AllAgree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKKFkDxNwbTu",
        "outputId": "a553632b-b3a0-492a-caa6-e79a47b3da3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:37<00:00,  1.31it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:02<00:00,  5.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 39.9046, Val Loss: 8.2508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:20<00:00,  2.42it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00, 11.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 38.0406, Val Loss: 8.8048\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2515\n",
            "Precision: 0.0632\n",
            "Recall: 0.2515\n",
            "F1 Score: 0.1011\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.25      1.00      0.40       387\n",
            "\n",
            "    accuracy                           0.25      1539\n",
            "   macro avg       0.08      0.33      0.13      1539\n",
            "weighted avg       0.06      0.25      0.10      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2519\n",
            "Precision: 0.0635\n",
            "Recall: 0.2519\n",
            "F1 Score: 0.1014\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.25      1.00      0.40        97\n",
            "\n",
            "    accuracy                           0.25       385\n",
            "   macro avg       0.08      0.33      0.13       385\n",
            "weighted avg       0.06      0.25      0.10       385\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2529\n",
            "Precision: 0.0640\n",
            "Recall: 0.2529\n",
            "F1 Score: 0.1021\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.25      1.00      0.40        86\n",
            "\n",
            "    accuracy                           0.25       340\n",
            "   macro avg       0.08      0.33      0.13       340\n",
            "weighted avg       0.06      0.25      0.10       340\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:05<00:00,  9.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 37.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 38.4088, Val Loss: 8.7978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:06<00:00,  7.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 24.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 21.1557, Val Loss: 4.0683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:07<00:00,  6.15it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 25.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 7.7430, Val Loss: 4.7309\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3717\n",
            "Precision: 0.1911\n",
            "Recall: 0.3717\n",
            "F1 Score: 0.2325\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.88      0.90      0.89       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.29      1.00      0.45       387\n",
            "\n",
            "    accuracy                           0.37      1539\n",
            "   macro avg       0.39      0.63      0.45      1539\n",
            "weighted avg       0.19      0.37      0.23      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3455\n",
            "Precision: 0.1800\n",
            "Recall: 0.3455\n",
            "F1 Score: 0.2129\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.71      0.76        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.28      0.99      0.44        97\n",
            "\n",
            "    accuracy                           0.35       385\n",
            "   macro avg       0.36      0.57      0.40       385\n",
            "weighted avg       0.18      0.35      0.21       385\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3500\n",
            "Precision: 0.1870\n",
            "Recall: 0.3500\n",
            "F1 Score: 0.2174\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.87      0.73      0.80        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.28      1.00      0.44        86\n",
            "\n",
            "    accuracy                           0.35       340\n",
            "   macro avg       0.38      0.58      0.41       340\n",
            "weighted avg       0.19      0.35      0.22       340\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.35, 0.18696768703996064, 0.35, 0.21737307020684313)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.50d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_75Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=50)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '50dim_75Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '50dim_75Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7AUguJn1Itj",
        "outputId": "8545265f-586a-461a-8617-656c10dae14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:24<00:00,  3.04it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:01<00:00, 10.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 56.6719, Val Loss: 12.9030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:37<00:00,  1.98it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:01<00:00, 17.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 52.5867, Val Loss: 12.8824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 74/74 [00:25<00:00,  2.91it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 19/19 [00:01<00:00, 15.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 53.3273, Val Loss: 11.9674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 74/74 [00:22<00:00,  3.23it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 19/19 [00:01<00:00, 10.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 51.6564, Val Loss: 11.9001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 74/74 [00:29<00:00,  2.55it/s]\n",
            "Validation Epoch 5/20: 100%|██████████| 19/19 [00:01<00:00, 10.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Train Loss: 51.2360, Val Loss: 12.1180\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2568\n",
            "Precision: 0.0660\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1050\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       286\n",
            "     neutral       0.00      0.00      0.00      1459\n",
            "    positive       0.26      1.00      0.41       603\n",
            "\n",
            "    accuracy                           0.26      2348\n",
            "   macro avg       0.09      0.33      0.14      2348\n",
            "weighted avg       0.07      0.26      0.10      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2572\n",
            "Precision: 0.0662\n",
            "Recall: 0.2572\n",
            "F1 Score: 0.1053\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.26      1.00      0.41       151\n",
            "\n",
            "    accuracy                           0.26       587\n",
            "   macro avg       0.09      0.33      0.14       587\n",
            "weighted avg       0.07      0.26      0.11       587\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2568\n",
            "Precision: 0.0659\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1049\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.26      1.00      0.41       133\n",
            "\n",
            "    accuracy                           0.26       518\n",
            "   macro avg       0.09      0.33      0.14       518\n",
            "weighted avg       0.07      0.26      0.10       518\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:10<00:00,  7.34it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:00<00:00, 40.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 50.4074, Val Loss: 10.2995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:08<00:00,  8.26it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:00<00:00, 24.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 22.1135, Val Loss: 6.3130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 74/74 [00:12<00:00,  5.95it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 19/19 [00:00<00:00, 24.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 9.7614, Val Loss: 5.6009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 74/74 [00:09<00:00,  7.77it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 19/19 [00:00<00:00, 39.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 3.6610, Val Loss: 9.6715\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3722\n",
            "Precision: 0.1732\n",
            "Recall: 0.3722\n",
            "F1 Score: 0.2236\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.95      0.86       286\n",
            "     neutral       0.00      0.00      0.00      1459\n",
            "    positive       0.30      1.00      0.46       603\n",
            "\n",
            "    accuracy                           0.37      2348\n",
            "   macro avg       0.36      0.65      0.44      2348\n",
            "weighted avg       0.17      0.37      0.22      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3305\n",
            "Precision: 0.1569\n",
            "Recall: 0.3305\n",
            "F1 Score: 0.1933\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.69      0.62      0.65        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.29      0.99      0.45       151\n",
            "\n",
            "    accuracy                           0.33       587\n",
            "   macro avg       0.32      0.54      0.37       587\n",
            "weighted avg       0.16      0.33      0.19       587\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3205\n",
            "Precision: 0.1496\n",
            "Recall: 0.3205\n",
            "F1 Score: 0.1842\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.63      0.54      0.58        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.28      0.99      0.44       133\n",
            "\n",
            "    accuracy                           0.32       518\n",
            "   macro avg       0.30      0.51      0.34       518\n",
            "weighted avg       0.15      0.32      0.18       518\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3204633204633205,\n",
              " 0.14961944703324015,\n",
              " 0.3204633204633205,\n",
              " 0.1842267470408174)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.50d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_66Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=50)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '50dim_66Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "\n",
        "model_run = model_run = '50dim_66Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhZAt79I1PzY",
        "outputId": "c5b9b3b8-e489-43dd-ca88-d2ea395d9da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:33<00:00,  2.65it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:02<00:00,  9.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 66.5124, Val Loss: 14.7736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:33<00:00,  2.67it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:02<00:00,  9.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 65.4279, Val Loss: 14.5354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 90/90 [00:33<00:00,  2.68it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 23/23 [00:01<00:00, 15.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 62.9939, Val Loss: 14.2621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 90/90 [00:39<00:00,  2.28it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 23/23 [00:02<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 62.8865, Val Loss: 14.1341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 90/90 [00:32<00:00,  2.73it/s]\n",
            "Validation Epoch 5/20: 100%|██████████| 23/23 [00:01<00:00, 15.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Train Loss: 62.7566, Val Loss: 15.0996\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2769\n",
            "Precision: 0.0767\n",
            "Recall: 0.2769\n",
            "F1 Score: 0.1201\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.28      1.00      0.43       794\n",
            "\n",
            "    accuracy                           0.28      2867\n",
            "   macro avg       0.09      0.33      0.14      2867\n",
            "weighted avg       0.08      0.28      0.12      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2775\n",
            "Precision: 0.0770\n",
            "Recall: 0.2775\n",
            "F1 Score: 0.1206\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.28      1.00      0.43       199\n",
            "\n",
            "    accuracy                           0.28       717\n",
            "   macro avg       0.09      0.33      0.14       717\n",
            "weighted avg       0.08      0.28      0.12       717\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2765\n",
            "Precision: 0.0764\n",
            "Recall: 0.2765\n",
            "F1 Score: 0.1198\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.28      1.00      0.43       175\n",
            "\n",
            "    accuracy                           0.28       633\n",
            "   macro avg       0.09      0.33      0.14       633\n",
            "weighted avg       0.08      0.28      0.12       633\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:17<00:00,  5.18it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:01<00:00, 21.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 55.0059, Val Loss: 10.1934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:12<00:00,  7.47it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:00<00:00, 29.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 21.2167, Val Loss: 8.4151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 90/90 [00:17<00:00,  5.20it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 23/23 [00:01<00:00, 21.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 6.2597, Val Loss: 12.5562\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3907\n",
            "Precision: 0.1712\n",
            "Recall: 0.3907\n",
            "F1 Score: 0.2321\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.64      0.94      0.76       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.34      1.00      0.50       794\n",
            "\n",
            "    accuracy                           0.39      2867\n",
            "   macro avg       0.33      0.64      0.42      2867\n",
            "weighted avg       0.17      0.39      0.23      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3529\n",
            "Precision: 0.1506\n",
            "Recall: 0.3529\n",
            "F1 Score: 0.2052\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.70      0.59        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.32      0.96      0.48       199\n",
            "\n",
            "    accuracy                           0.35       717\n",
            "   macro avg       0.28      0.56      0.36       717\n",
            "weighted avg       0.15      0.35      0.21       717\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3602\n",
            "Precision: 0.1534\n",
            "Recall: 0.3602\n",
            "F1 Score: 0.2096\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.52      0.74      0.61        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.33      0.98      0.49       175\n",
            "\n",
            "    accuracy                           0.36       633\n",
            "   macro avg       0.28      0.57      0.37       633\n",
            "weighted avg       0.15      0.36      0.21       633\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.36018957345971564,\n",
              " 0.15342491821699455,\n",
              " 0.36018957345971564,\n",
              " 0.20961479011680995)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.50d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_50Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=50)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '50dim_50Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "\n",
        "model_run = model_run = '50dim_50Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAyTtq_31VpJ",
        "outputId": "94cc4c39-9d07-4400-a531-4ed5c1708314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:38<00:00,  2.70it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:02<00:00,  9.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 79.4954, Val Loss: 16.7469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:41<00:00,  2.51it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:02<00:00,  8.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 71.2206, Val Loss: 16.1373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 103/103 [00:39<00:00,  2.58it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 26/26 [00:02<00:00,  9.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 72.1461, Val Loss: 15.7710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 103/103 [00:39<00:00,  2.61it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 26/26 [00:01<00:00, 15.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 73.9360, Val Loss: 16.4521\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2813\n",
            "Precision: 0.0791\n",
            "Recall: 0.2813\n",
            "F1 Score: 0.1235\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.28      1.00      0.44       927\n",
            "\n",
            "    accuracy                           0.28      3295\n",
            "   macro avg       0.09      0.33      0.15      3295\n",
            "weighted avg       0.08      0.28      0.12      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2816\n",
            "Precision: 0.0793\n",
            "Recall: 0.2816\n",
            "F1 Score: 0.1237\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.28      1.00      0.44       232\n",
            "\n",
            "    accuracy                           0.28       824\n",
            "   macro avg       0.09      0.33      0.15       824\n",
            "weighted avg       0.08      0.28      0.12       824\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2806\n",
            "Precision: 0.0787\n",
            "Recall: 0.2806\n",
            "F1 Score: 0.1230\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.28      1.00      0.44       204\n",
            "\n",
            "    accuracy                           0.28       727\n",
            "   macro avg       0.09      0.33      0.15       727\n",
            "weighted avg       0.08      0.28      0.12       727\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:19<00:00,  5.34it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:00<00:00, 35.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 60.1497, Val Loss: 8.2039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:15<00:00,  6.79it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:01<00:00, 21.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 19.3747, Val Loss: 5.2617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 103/103 [00:17<00:00,  5.84it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 26/26 [00:00<00:00, 33.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 7.0571, Val Loss: 7.3468\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.4021\n",
            "Precision: 0.1715\n",
            "Recall: 0.4021\n",
            "F1 Score: 0.2376\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.98      0.72       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.36      1.00      0.52       927\n",
            "\n",
            "    accuracy                           0.40      3295\n",
            "   macro avg       0.31      0.66      0.42      3295\n",
            "weighted avg       0.17      0.40      0.24      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3653\n",
            "Precision: 0.1535\n",
            "Recall: 0.3653\n",
            "F1 Score: 0.2138\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.47      0.81      0.59       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.34      0.94      0.50       232\n",
            "\n",
            "    accuracy                           0.37       824\n",
            "   macro avg       0.27      0.58      0.36       824\n",
            "weighted avg       0.15      0.37      0.21       824\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3576\n",
            "Precision: 0.1484\n",
            "Recall: 0.3576\n",
            "F1 Score: 0.2077\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.43      0.75      0.54        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.34      0.94      0.50       204\n",
            "\n",
            "    accuracy                           0.36       727\n",
            "   macro avg       0.26      0.56      0.35       727\n",
            "weighted avg       0.15      0.36      0.21       727\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3576341127922971,\n",
              " 0.1483851244464703,\n",
              " 0.3576341127922971,\n",
              " 0.20766919200917963)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 100 Dim, All Agreements"
      ],
      "metadata": {
        "id": "ReNTA8kK1a8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.100d.txt'\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_AllAgree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=100)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '100dim_AllAgree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '100dim_AllAgree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "911cd980-e964-4d21-ddc0-b973d5d545c0",
        "id": "6efiZ_KF1kWl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:13<00:00,  3.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 13.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 40.7989, Val Loss: 7.9966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:20<00:00,  2.40it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 36.6721, Val Loss: 7.8807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:19<00:00,  2.55it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 17.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 35.3222, Val Loss: 8.7761\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2515\n",
            "Precision: 0.0632\n",
            "Recall: 0.2515\n",
            "F1 Score: 0.1011\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.25      1.00      0.40       387\n",
            "\n",
            "    accuracy                           0.25      1539\n",
            "   macro avg       0.08      0.33      0.13      1539\n",
            "weighted avg       0.06      0.25      0.10      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2519\n",
            "Precision: 0.0635\n",
            "Recall: 0.2519\n",
            "F1 Score: 0.1014\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.25      1.00      0.40        97\n",
            "\n",
            "    accuracy                           0.25       385\n",
            "   macro avg       0.08      0.33      0.13       385\n",
            "weighted avg       0.06      0.25      0.10       385\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2529\n",
            "Precision: 0.0640\n",
            "Recall: 0.2529\n",
            "F1 Score: 0.1021\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.25      1.00      0.40        86\n",
            "\n",
            "    accuracy                           0.25       340\n",
            "   macro avg       0.08      0.33      0.13       340\n",
            "weighted avg       0.06      0.25      0.10       340\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:06<00:00,  7.18it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 23.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 35.4895, Val Loss: 9.6175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:08<00:00,  5.57it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 23.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 16.4370, Val Loss: 3.8873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:08<00:00,  5.66it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 39.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 3.9956, Val Loss: 5.5042\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3808\n",
            "Precision: 0.1569\n",
            "Recall: 0.3808\n",
            "F1 Score: 0.2194\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      1.00      0.44       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.48      0.98      0.64       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.25      0.66      0.36      1539\n",
            "weighted avg       0.16      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3143\n",
            "Precision: 0.1325\n",
            "Recall: 0.3143\n",
            "F1 Score: 0.1826\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.23      0.90      0.37        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.40      0.76      0.52        97\n",
            "\n",
            "    accuracy                           0.31       385\n",
            "   macro avg       0.21      0.56      0.30       385\n",
            "weighted avg       0.13      0.31      0.18       385\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3206\n",
            "Precision: 0.1292\n",
            "Recall: 0.3206\n",
            "F1 Score: 0.1817\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.26      0.96      0.41        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.37      0.77      0.50        86\n",
            "\n",
            "    accuracy                           0.32       340\n",
            "   macro avg       0.21      0.57      0.31       340\n",
            "weighted avg       0.13      0.32      0.18       340\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3205882352941177,\n",
              " 0.12923224192453317,\n",
              " 0.3205882352941177,\n",
              " 0.1816743156753781)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.100d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_75Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=100)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '100dim_75Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "\n",
        "model_run = model_run = '100dim_75Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgpCQWcV1kWu",
        "outputId": "baaf9a08-b14d-4d9b-efbe-a45940e7f3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:36<00:00,  2.04it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:01<00:00, 10.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 55.7424, Val Loss: 11.9977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:23<00:00,  3.10it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:01<00:00, 10.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 57.4218, Val Loss: 12.0805\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2568\n",
            "Precision: 0.0660\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1050\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       286\n",
            "     neutral       0.00      0.00      0.00      1459\n",
            "    positive       0.26      1.00      0.41       603\n",
            "\n",
            "    accuracy                           0.26      2348\n",
            "   macro avg       0.09      0.33      0.14      2348\n",
            "weighted avg       0.07      0.26      0.10      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2572\n",
            "Precision: 0.0662\n",
            "Recall: 0.2572\n",
            "F1 Score: 0.1053\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.26      1.00      0.41       151\n",
            "\n",
            "    accuracy                           0.26       587\n",
            "   macro avg       0.09      0.33      0.14       587\n",
            "weighted avg       0.07      0.26      0.11       587\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2568\n",
            "Precision: 0.0659\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1049\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.26      1.00      0.41       133\n",
            "\n",
            "    accuracy                           0.26       518\n",
            "   macro avg       0.09      0.33      0.14       518\n",
            "weighted avg       0.07      0.26      0.10       518\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:08<00:00,  8.24it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:00<00:00, 25.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 46.0692, Val Loss: 6.2695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:13<00:00,  5.36it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:00<00:00, 22.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 14.0445, Val Loss: 6.5682\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3612\n",
            "Precision: 0.8049\n",
            "Recall: 0.3612\n",
            "F1 Score: 0.2230\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.89      0.86      0.88       286\n",
            "     neutral       1.00      0.00      0.00      1459\n",
            "    positive       0.29      1.00      0.45       603\n",
            "\n",
            "    accuracy                           0.36      2348\n",
            "   macro avg       0.73      0.62      0.44      2348\n",
            "weighted avg       0.80      0.36      0.22      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3305\n",
            "Precision: 0.1745\n",
            "Recall: 0.3305\n",
            "F1 Score: 0.1997\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.63      0.73        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.28      0.99      0.44       151\n",
            "\n",
            "    accuracy                           0.33       587\n",
            "   macro avg       0.38      0.54      0.39       587\n",
            "weighted avg       0.17      0.33      0.20       587\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3340\n",
            "Precision: 0.1686\n",
            "Recall: 0.3340\n",
            "F1 Score: 0.1999\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.65      0.71        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.28      0.99      0.44       133\n",
            "\n",
            "    accuracy                           0.33       518\n",
            "   macro avg       0.36      0.55      0.38       518\n",
            "weighted avg       0.17      0.33      0.20       518\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.333976833976834,\n",
              " 0.16862335263622816,\n",
              " 0.333976833976834,\n",
              " 0.19988307971177974)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.100d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_66Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=100)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '100dim_66Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '100dim_66Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcEdGXqF1kWu",
        "outputId": "e7bdc679-7cfd-4b68-c3bc-de1ce69d437d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:41<00:00,  2.17it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:02<00:00, 10.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 64.9892, Val Loss: 15.2590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:36<00:00,  2.48it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:02<00:00,  9.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 63.9135, Val Loss: 13.9013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 90/90 [00:35<00:00,  2.54it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 23/23 [00:02<00:00,  8.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 62.4420, Val Loss: 14.1601\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2769\n",
            "Precision: 0.0767\n",
            "Recall: 0.2769\n",
            "F1 Score: 0.1201\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.28      1.00      0.43       794\n",
            "\n",
            "    accuracy                           0.28      2867\n",
            "   macro avg       0.09      0.33      0.14      2867\n",
            "weighted avg       0.08      0.28      0.12      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2775\n",
            "Precision: 0.0770\n",
            "Recall: 0.2775\n",
            "F1 Score: 0.1206\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.28      1.00      0.43       199\n",
            "\n",
            "    accuracy                           0.28       717\n",
            "   macro avg       0.09      0.33      0.14       717\n",
            "weighted avg       0.08      0.28      0.12       717\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2765\n",
            "Precision: 0.0764\n",
            "Recall: 0.2765\n",
            "F1 Score: 0.1198\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.28      1.00      0.43       175\n",
            "\n",
            "    accuracy                           0.28       633\n",
            "   macro avg       0.09      0.33      0.14       633\n",
            "weighted avg       0.08      0.28      0.12       633\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:18<00:00,  4.75it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:01<00:00, 18.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 62.4985, Val Loss: 8.1138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:14<00:00,  6.29it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:00<00:00, 31.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 19.3041, Val Loss: 6.4685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 90/90 [00:18<00:00,  4.77it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 23/23 [00:01<00:00, 19.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 5.9754, Val Loss: 7.6700\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3931\n",
            "Precision: 0.1579\n",
            "Recall: 0.3931\n",
            "F1 Score: 0.2250\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      1.00      0.62       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.37      0.98      0.54       794\n",
            "\n",
            "    accuracy                           0.39      2867\n",
            "   macro avg       0.27      0.66      0.39      2867\n",
            "weighted avg       0.16      0.39      0.22      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3584\n",
            "Precision: 0.1433\n",
            "Recall: 0.3584\n",
            "F1 Score: 0.2044\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.80      0.51        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.35      0.94      0.51       199\n",
            "\n",
            "    accuracy                           0.36       717\n",
            "   macro avg       0.24      0.58      0.34       717\n",
            "weighted avg       0.14      0.36      0.20       717\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3728\n",
            "Precision: 0.1490\n",
            "Recall: 0.3728\n",
            "F1 Score: 0.2127\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.40      0.91      0.56        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.36      0.95      0.52       175\n",
            "\n",
            "    accuracy                           0.37       633\n",
            "   macro avg       0.26      0.62      0.36       633\n",
            "weighted avg       0.15      0.37      0.21       633\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.37282780410742494,\n",
              " 0.14898616631470038,\n",
              " 0.37282780410742494,\n",
              " 0.2126636066226319)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.100d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_50Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=100)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '100dim_50Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '100dim_50Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VDZ3VOd1kWv",
        "outputId": "f3a2a4ee-8018-4b31-9281-a11b9af2f8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:48<00:00,  2.11it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:02<00:00,  8.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 75.5496, Val Loss: 15.8580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:39<00:00,  2.63it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:01<00:00, 14.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 73.5543, Val Loss: 15.9520\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2813\n",
            "Precision: 0.0791\n",
            "Recall: 0.2813\n",
            "F1 Score: 0.1235\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.28      1.00      0.44       927\n",
            "\n",
            "    accuracy                           0.28      3295\n",
            "   macro avg       0.09      0.33      0.15      3295\n",
            "weighted avg       0.08      0.28      0.12      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2816\n",
            "Precision: 0.0793\n",
            "Recall: 0.2816\n",
            "F1 Score: 0.1237\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.28      1.00      0.44       232\n",
            "\n",
            "    accuracy                           0.28       824\n",
            "   macro avg       0.09      0.33      0.15       824\n",
            "weighted avg       0.08      0.28      0.12       824\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2806\n",
            "Precision: 0.0787\n",
            "Recall: 0.2806\n",
            "F1 Score: 0.1230\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.28      1.00      0.44       204\n",
            "\n",
            "    accuracy                           0.28       727\n",
            "   macro avg       0.09      0.33      0.15       727\n",
            "weighted avg       0.08      0.28      0.12       727\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:18<00:00,  5.59it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:00<00:00, 31.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 57.2880, Val Loss: 6.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:20<00:00,  4.95it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:01<00:00, 18.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 15.7329, Val Loss: 6.7354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 103/103 [00:16<00:00,  6.27it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 26/26 [00:01<00:00, 19.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 6.1111, Val Loss: 8.8013\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.4033\n",
            "Precision: 0.1637\n",
            "Recall: 0.4033\n",
            "F1 Score: 0.2329\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.39      0.99      0.56       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.41      0.99      0.58       927\n",
            "\n",
            "    accuracy                           0.40      3295\n",
            "   macro avg       0.27      0.66      0.38      3295\n",
            "weighted avg       0.16      0.40      0.23      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3665\n",
            "Precision: 0.1490\n",
            "Recall: 0.3665\n",
            "F1 Score: 0.2118\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.35      0.85      0.50       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.37      0.92      0.53       232\n",
            "\n",
            "    accuracy                           0.37       824\n",
            "   macro avg       0.24      0.59      0.34       824\n",
            "weighted avg       0.15      0.37      0.21       824\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3590\n",
            "Precision: 0.1454\n",
            "Recall: 0.3590\n",
            "F1 Score: 0.2070\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      0.77      0.46        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.37      0.94      0.53       204\n",
            "\n",
            "    accuracy                           0.36       727\n",
            "   macro avg       0.23      0.57      0.33       727\n",
            "weighted avg       0.15      0.36      0.21       727\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.35900962861072905,\n",
              " 0.14543255773967564,\n",
              " 0.35900962861072905,\n",
              " 0.20697658288159593)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 200 Dim"
      ],
      "metadata": {
        "id": "BCpH8jzU2QLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.200d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_AllAgree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=200)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '200dim_AllAgree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '200dim_AllAgree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c7ec18-f5f3-40dd-941b-b53e1c578818",
        "id": "r-X_Exra2s1q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:17<00:00,  2.81it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 15.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 1/20 - Train Loss: 40.4519, Val Loss: 7.7781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:19<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:01<00:00,  8.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 35.3517, Val Loss: 8.2022\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2515\n",
            "Precision: 0.0632\n",
            "Recall: 0.2515\n",
            "F1 Score: 0.1011\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.25      1.00      0.40       387\n",
            "\n",
            "    accuracy                           0.25      1539\n",
            "   macro avg       0.08      0.33      0.13      1539\n",
            "weighted avg       0.06      0.25      0.10      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2519\n",
            "Precision: 0.0635\n",
            "Recall: 0.2519\n",
            "F1 Score: 0.1014\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.25      1.00      0.40        97\n",
            "\n",
            "    accuracy                           0.25       385\n",
            "   macro avg       0.08      0.33      0.13       385\n",
            "weighted avg       0.06      0.25      0.10       385\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2529\n",
            "Precision: 0.0640\n",
            "Recall: 0.2529\n",
            "F1 Score: 0.1021\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.25      1.00      0.40        86\n",
            "\n",
            "    accuracy                           0.25       340\n",
            "   macro avg       0.08      0.33      0.13       340\n",
            "weighted avg       0.06      0.25      0.10       340\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:10<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 16.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 33.5647, Val Loss: 7.2050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:07<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 30.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 12.2292, Val Loss: 2.8452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:07<00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 18.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 3.4770, Val Loss: 3.5583\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3840\n",
            "Precision: 0.1511\n",
            "Recall: 0.3840\n",
            "F1 Score: 0.2159\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.47      1.00      0.64       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.35      0.99      0.52       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.27      0.66      0.39      1539\n",
            "weighted avg       0.15      0.38      0.22      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3455\n",
            "Precision: 0.1363\n",
            "Recall: 0.3455\n",
            "F1 Score: 0.1947\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.42      0.90      0.58        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.31      0.89      0.46        97\n",
            "\n",
            "    accuracy                           0.35       385\n",
            "   macro avg       0.25      0.60      0.35       385\n",
            "weighted avg       0.14      0.35      0.19       385\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3647\n",
            "Precision: 0.1474\n",
            "Recall: 0.3647\n",
            "F1 Score: 0.2079\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.51      0.98      0.67        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.31      0.93      0.47        86\n",
            "\n",
            "    accuracy                           0.36       340\n",
            "   macro avg       0.28      0.64      0.38       340\n",
            "weighted avg       0.15      0.36      0.21       340\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.36470588235294116,\n",
              " 0.1473819705505348,\n",
              " 0.36470588235294116,\n",
              " 0.20793998784965265)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.200d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_75Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=200)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '200dim_75Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '200dim_75Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szUhZPFG2s1z",
        "outputId": "a6804a9a-9199-415b-ca4d-53f75bf5b54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:27<00:00,  2.74it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:01<00:00, 10.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 56.7511, Val Loss: 12.4644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:27<00:00,  2.68it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:01<00:00, 15.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 54.3254, Val Loss: 12.5088\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2568\n",
            "Precision: 0.0660\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1050\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       286\n",
            "     neutral       0.00      0.00      0.00      1459\n",
            "    positive       0.26      1.00      0.41       603\n",
            "\n",
            "    accuracy                           0.26      2348\n",
            "   macro avg       0.09      0.33      0.14      2348\n",
            "weighted avg       0.07      0.26      0.10      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2572\n",
            "Precision: 0.0662\n",
            "Recall: 0.2572\n",
            "F1 Score: 0.1053\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.26      1.00      0.41       151\n",
            "\n",
            "    accuracy                           0.26       587\n",
            "   macro avg       0.09      0.33      0.14       587\n",
            "weighted avg       0.07      0.26      0.11       587\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2568\n",
            "Precision: 0.0659\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1049\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.26      1.00      0.41       133\n",
            "\n",
            "    accuracy                           0.26       518\n",
            "   macro avg       0.09      0.33      0.14       518\n",
            "weighted avg       0.07      0.26      0.10       518\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:14<00:00,  5.00it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:00<00:00, 29.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 57.7308, Val Loss: 9.4429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:13<00:00,  5.54it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:00<00:00, 19.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 16.9761, Val Loss: 8.7012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 74/74 [00:15<00:00,  4.82it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 19/19 [00:00<00:00, 28.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 5.4604, Val Loss: 7.4675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 74/74 [00:12<00:00,  5.86it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 19/19 [00:01<00:00, 18.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 1.5095, Val Loss: 11.7387\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3778\n",
            "Precision: 0.1540\n",
            "Recall: 0.3778\n",
            "F1 Score: 0.2151\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      1.00      0.74       286\n",
            "     neutral       0.00      0.00      0.00      1459\n",
            "    positive       0.32      1.00      0.49       603\n",
            "\n",
            "    accuracy                           0.38      2348\n",
            "   macro avg       0.30      0.67      0.41      2348\n",
            "weighted avg       0.15      0.38      0.22      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3288\n",
            "Precision: 0.1320\n",
            "Recall: 0.3288\n",
            "F1 Score: 0.1835\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      0.68      0.54        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.30      0.96      0.46       151\n",
            "\n",
            "    accuracy                           0.33       587\n",
            "   macro avg       0.25      0.55      0.33       587\n",
            "weighted avg       0.13      0.33      0.18       587\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3436\n",
            "Precision: 0.1371\n",
            "Recall: 0.3436\n",
            "F1 Score: 0.1922\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.47      0.76      0.58        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.31      0.98      0.47       133\n",
            "\n",
            "    accuracy                           0.34       518\n",
            "   macro avg       0.26      0.58      0.35       518\n",
            "weighted avg       0.14      0.34      0.19       518\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3436293436293436,\n",
              " 0.13710786467834074,\n",
              " 0.3436293436293436,\n",
              " 0.19215429482776245)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.200d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_66Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=200)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '200dim_66Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '200dim_66Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cki-AP_2s10",
        "outputId": "4045937c-43f4-4801-b5ee-cc5fbc39a376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:37<00:00,  2.38it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:01<00:00, 13.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 66.4920, Val Loss: 14.1887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:36<00:00,  2.49it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:02<00:00,  8.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 62.8651, Val Loss: 14.2707\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2769\n",
            "Precision: 0.0767\n",
            "Recall: 0.2769\n",
            "F1 Score: 0.1201\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.28      1.00      0.43       794\n",
            "\n",
            "    accuracy                           0.28      2867\n",
            "   macro avg       0.09      0.33      0.14      2867\n",
            "weighted avg       0.08      0.28      0.12      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2775\n",
            "Precision: 0.0770\n",
            "Recall: 0.2775\n",
            "F1 Score: 0.1206\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.28      1.00      0.43       199\n",
            "\n",
            "    accuracy                           0.28       717\n",
            "   macro avg       0.09      0.33      0.14       717\n",
            "weighted avg       0.08      0.28      0.12       717\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2765\n",
            "Precision: 0.0764\n",
            "Recall: 0.2765\n",
            "F1 Score: 0.1198\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.28      1.00      0.43       175\n",
            "\n",
            "    accuracy                           0.28       633\n",
            "   macro avg       0.09      0.33      0.14       633\n",
            "weighted avg       0.08      0.28      0.12       633\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:17<00:00,  5.15it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:01<00:00, 16.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 58.5073, Val Loss: 7.3988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:20<00:00,  4.41it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:00<00:00, 26.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 17.1197, Val Loss: 7.5660\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3931\n",
            "Precision: 0.1601\n",
            "Recall: 0.3931\n",
            "F1 Score: 0.2265\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      0.99      0.66       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.36      0.98      0.53       794\n",
            "\n",
            "    accuracy                           0.39      2867\n",
            "   macro avg       0.28      0.66      0.40      2867\n",
            "weighted avg       0.16      0.39      0.23      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3487\n",
            "Precision: 0.1414\n",
            "Recall: 0.3487\n",
            "F1 Score: 0.1995\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.40      0.74      0.52        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.33      0.93      0.49       199\n",
            "\n",
            "    accuracy                           0.35       717\n",
            "   macro avg       0.25      0.56      0.34       717\n",
            "weighted avg       0.14      0.35      0.20       717\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3602\n",
            "Precision: 0.1458\n",
            "Recall: 0.3602\n",
            "F1 Score: 0.2064\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.43      0.83      0.56        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.34      0.94      0.50       175\n",
            "\n",
            "    accuracy                           0.36       633\n",
            "   macro avg       0.26      0.59      0.35       633\n",
            "weighted avg       0.15      0.36      0.21       633\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.36018957345971564,\n",
              " 0.14577191656936145,\n",
              " 0.36018957345971564,\n",
              " 0.20640212441680042)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.200d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_50Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=200)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '200dim_50Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '200dim_50Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD3c2ShP2s10",
        "outputId": "b1c26645-4003-41b4-dcfc-07e2f0b6707e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:50<00:00,  2.05it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:03<00:00,  8.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 78.3121, Val Loss: 16.3678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:46<00:00,  2.24it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:01<00:00, 13.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 75.7813, Val Loss: 16.3306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 103/103 [00:44<00:00,  2.33it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 26/26 [00:03<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 72.3829, Val Loss: 16.2527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 103/103 [00:44<00:00,  2.29it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 26/26 [00:01<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 72.2477, Val Loss: 16.7799\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2813\n",
            "Precision: 0.0791\n",
            "Recall: 0.2813\n",
            "F1 Score: 0.1235\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.28      1.00      0.44       927\n",
            "\n",
            "    accuracy                           0.28      3295\n",
            "   macro avg       0.09      0.33      0.15      3295\n",
            "weighted avg       0.08      0.28      0.12      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2816\n",
            "Precision: 0.0793\n",
            "Recall: 0.2816\n",
            "F1 Score: 0.1237\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.28      1.00      0.44       232\n",
            "\n",
            "    accuracy                           0.28       824\n",
            "   macro avg       0.09      0.33      0.15       824\n",
            "weighted avg       0.08      0.28      0.12       824\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2806\n",
            "Precision: 0.0787\n",
            "Recall: 0.2806\n",
            "F1 Score: 0.1230\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.28      1.00      0.44       204\n",
            "\n",
            "    accuracy                           0.28       727\n",
            "   macro avg       0.09      0.33      0.15       727\n",
            "weighted avg       0.08      0.28      0.12       727\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:27<00:00,  3.69it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:01<00:00, 15.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 67.8905, Val Loss: 8.6803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:27<00:00,  3.73it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:00<00:00, 26.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 19.5626, Val Loss: 9.4210\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3997\n",
            "Precision: 0.1700\n",
            "Recall: 0.3997\n",
            "F1 Score: 0.2359\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.98      0.72       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.35      0.99      0.52       927\n",
            "\n",
            "    accuracy                           0.40      3295\n",
            "   macro avg       0.31      0.66      0.41      3295\n",
            "weighted avg       0.17      0.40      0.24      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3653\n",
            "Precision: 0.1537\n",
            "Recall: 0.3653\n",
            "F1 Score: 0.2142\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.48      0.83      0.61       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.33      0.93      0.49       232\n",
            "\n",
            "    accuracy                           0.37       824\n",
            "   macro avg       0.27      0.59      0.37       824\n",
            "weighted avg       0.15      0.37      0.21       824\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3631\n",
            "Precision: 0.1526\n",
            "Recall: 0.3631\n",
            "F1 Score: 0.2119\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.46      0.76      0.58        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.34      0.96      0.50       204\n",
            "\n",
            "    accuracy                           0.36       727\n",
            "   macro avg       0.27      0.57      0.36       727\n",
            "weighted avg       0.15      0.36      0.21       727\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3631361760660248,\n",
              " 0.15263338240588742,\n",
              " 0.3631361760660248,\n",
              " 0.21191764846600084)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 300 Dim"
      ],
      "metadata": {
        "id": "lqaqAMLd2USR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.300d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_AllAgree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=300)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '300dim_AllAgree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '300dim_AllAgree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e0ede1-b895-49bc-b62d-e7908443efd9",
        "id": "KWDENYrn2XkM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:20<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:01<00:00, 10.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 39.3277, Val Loss: 8.9601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:28<00:00,  1.73it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 14.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 38.2387, Val Loss: 7.8583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:20<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in loss computation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:01<00:00,  8.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 3/20 - Train Loss: 34.3322, Val Loss: 7.8305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 49/49 [00:17<00:00,  2.85it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 13/13 [00:01<00:00,  8.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 4/20 - Train Loss: 37.0207, Val Loss: 7.7995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 49/49 [00:20<00:00,  2.39it/s]\n",
            "Validation Epoch 5/20: 100%|██████████| 13/13 [00:00<00:00, 15.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 5/20 - Train Loss: 34.1907, Val Loss: 7.6823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/20: 100%|██████████| 49/49 [00:20<00:00,  2.42it/s]\n",
            "Validation Epoch 6/20: 100%|██████████| 13/13 [00:01<00:00,  8.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 - Train Loss: 35.8010, Val Loss: 8.2383\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2515\n",
            "Precision: 0.0632\n",
            "Recall: 0.2515\n",
            "F1 Score: 0.1011\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.25      1.00      0.40       387\n",
            "\n",
            "    accuracy                           0.25      1539\n",
            "   macro avg       0.08      0.33      0.13      1539\n",
            "weighted avg       0.06      0.25      0.10      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2519\n",
            "Precision: 0.0635\n",
            "Recall: 0.2519\n",
            "F1 Score: 0.1014\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.25      1.00      0.40        97\n",
            "\n",
            "    accuracy                           0.25       385\n",
            "   macro avg       0.08      0.33      0.13       385\n",
            "weighted avg       0.06      0.25      0.10       385\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2529\n",
            "Precision: 0.0640\n",
            "Recall: 0.2529\n",
            "F1 Score: 0.1021\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.25      1.00      0.40        86\n",
            "\n",
            "    accuracy                           0.25       340\n",
            "   macro avg       0.08      0.33      0.13       340\n",
            "weighted avg       0.06      0.25      0.10       340\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 49/49 [00:12<00:00,  3.80it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 13/13 [00:00<00:00, 16.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 35.8157, Val Loss: 6.4138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 49/49 [00:12<00:00,  3.99it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 13/13 [00:00<00:00, 16.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in validation loss computation.\n",
            "Epoch 2/20 - Train Loss: 11.2453, Val Loss: 4.3128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 49/49 [00:12<00:00,  4.07it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 13/13 [00:00<00:00, 26.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 2.6765, Val Loss: 5.8157\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3795\n",
            "Precision: 0.1477\n",
            "Recall: 0.3795\n",
            "F1 Score: 0.2122\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.44      0.99      0.61       206\n",
            "     neutral       0.00      0.00      0.00       946\n",
            "    positive       0.35      0.98      0.52       387\n",
            "\n",
            "    accuracy                           0.38      1539\n",
            "   macro avg       0.26      0.66      0.38      1539\n",
            "weighted avg       0.15      0.38      0.21      1539\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3351\n",
            "Precision: 0.1298\n",
            "Recall: 0.3351\n",
            "F1 Score: 0.1870\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.32      0.87      0.47        52\n",
            "     neutral       0.00      0.00      0.00       236\n",
            "    positive       0.34      0.87      0.49        97\n",
            "\n",
            "    accuracy                           0.34       385\n",
            "   macro avg       0.22      0.58      0.32       385\n",
            "weighted avg       0.13      0.34      0.19       385\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3382\n",
            "Precision: 0.1313\n",
            "Recall: 0.3382\n",
            "F1 Score: 0.1887\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.82      0.51        45\n",
            "     neutral       0.00      0.00      0.00       209\n",
            "    positive       0.32      0.91      0.48        86\n",
            "\n",
            "    accuracy                           0.34       340\n",
            "   macro avg       0.23      0.58      0.33       340\n",
            "weighted avg       0.13      0.34      0.19       340\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3382352941176471,\n",
              " 0.13133001974837466,\n",
              " 0.3382352941176471,\n",
              " 0.18868389098758773)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.300d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_75Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=300)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '300dim_75Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '300dim_75Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lWev14w2XkM",
        "outputId": "66293610-4903-4785-f337-ef1ae577b96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sentence length:  81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:30<00:00,  2.40it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:02<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 57.8871, Val Loss: 12.0162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:30<00:00,  2.45it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:02<00:00,  8.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 53.3431, Val Loss: 11.7326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 74/74 [00:29<00:00,  2.47it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 19/19 [00:02<00:00,  9.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 57.4253, Val Loss: 11.9124\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2568\n",
            "Precision: 0.0660\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1050\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       286\n",
            "     neutral       0.00      0.00      0.00      1459\n",
            "    positive       0.26      1.00      0.41       603\n",
            "\n",
            "    accuracy                           0.26      2348\n",
            "   macro avg       0.09      0.33      0.14      2348\n",
            "weighted avg       0.07      0.26      0.10      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2572\n",
            "Precision: 0.0662\n",
            "Recall: 0.2572\n",
            "F1 Score: 0.1053\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.26      1.00      0.41       151\n",
            "\n",
            "    accuracy                           0.26       587\n",
            "   macro avg       0.09      0.33      0.14       587\n",
            "weighted avg       0.07      0.26      0.11       587\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2568\n",
            "Precision: 0.0659\n",
            "Recall: 0.2568\n",
            "F1 Score: 0.1049\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.26      1.00      0.41       133\n",
            "\n",
            "    accuracy                           0.26       518\n",
            "   macro avg       0.09      0.33      0.14       518\n",
            "weighted avg       0.07      0.26      0.10       518\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 74/74 [00:16<00:00,  4.44it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 19/19 [00:01<00:00, 16.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 43.5276, Val Loss: 9.1867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 74/74 [00:20<00:00,  3.54it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 19/19 [00:01<00:00, 15.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 12.2783, Val Loss: 6.7748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 74/74 [00:16<00:00,  4.45it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 19/19 [00:00<00:00, 24.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 4.0935, Val Loss: 8.7246\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3765\n",
            "Precision: 0.1623\n",
            "Recall: 0.3765\n",
            "F1 Score: 0.2197\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.68      0.99      0.80       286\n",
            "     neutral       0.00      0.00      0.00      1459\n",
            "    positive       0.31      1.00      0.47       603\n",
            "\n",
            "    accuracy                           0.38      2348\n",
            "   macro avg       0.33      0.66      0.43      2348\n",
            "weighted avg       0.16      0.38      0.22      2348\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3271\n",
            "Precision: 0.1451\n",
            "Recall: 0.3271\n",
            "F1 Score: 0.1885\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.59      0.65      0.62        71\n",
            "     neutral       0.00      0.00      0.00       365\n",
            "    positive       0.29      0.97      0.44       151\n",
            "\n",
            "    accuracy                           0.33       587\n",
            "   macro avg       0.29      0.54      0.35       587\n",
            "weighted avg       0.15      0.33      0.19       587\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3456\n",
            "Precision: 0.1527\n",
            "Recall: 0.3456\n",
            "F1 Score: 0.2005\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.63      0.75      0.68        63\n",
            "     neutral       0.00      0.00      0.00       322\n",
            "    positive       0.30      0.99      0.46       133\n",
            "\n",
            "    accuracy                           0.35       518\n",
            "   macro avg       0.31      0.58      0.38       518\n",
            "weighted avg       0.15      0.35      0.20       518\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.34555984555984554,\n",
              " 0.1527216155207126,\n",
              " 0.34555984555984554,\n",
              " 0.20052389345867608)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.300d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_66Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=300)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '300dim_66Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '300dim_66Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibwnK7O_2XkM",
        "outputId": "da5e8193-9298-4d64-c0fd-9d0ef3e43103"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:45<00:00,  1.96it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:01<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 66.4759, Val Loss: 14.3403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:03<00:00,  7.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 62.5776, Val Loss: 14.4238\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2769\n",
            "Precision: 0.0767\n",
            "Recall: 0.2769\n",
            "F1 Score: 0.1202\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.28      1.00      0.43       794\n",
            "\n",
            "    accuracy                           0.28      2867\n",
            "   macro avg       0.09      0.33      0.14      2867\n",
            "weighted avg       0.08      0.28      0.12      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2775\n",
            "Precision: 0.0770\n",
            "Recall: 0.2775\n",
            "F1 Score: 0.1206\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.28      1.00      0.43       199\n",
            "\n",
            "    accuracy                           0.28       717\n",
            "   macro avg       0.09      0.33      0.14       717\n",
            "weighted avg       0.08      0.28      0.12       717\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2765\n",
            "Precision: 0.0764\n",
            "Recall: 0.2765\n",
            "F1 Score: 0.1198\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.28      1.00      0.43       175\n",
            "\n",
            "    accuracy                           0.28       633\n",
            "   macro avg       0.09      0.33      0.14       633\n",
            "weighted avg       0.08      0.28      0.12       633\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 90/90 [00:25<00:00,  3.51it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 23/23 [00:01<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 51.3810, Val Loss: 8.4859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 90/90 [00:25<00:00,  3.46it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 23/23 [00:01<00:00, 13.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Train Loss: 13.8018, Val Loss: 8.1186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 90/90 [00:23<00:00,  3.79it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 23/23 [00:01<00:00, 22.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 4.6731, Val Loss: 11.9302\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.3976\n",
            "Precision: 0.1589\n",
            "Recall: 0.3976\n",
            "F1 Score: 0.2270\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.38      0.99      0.55       350\n",
            "     neutral       0.00      0.00      0.00      1723\n",
            "    positive       0.41      1.00      0.58       794\n",
            "\n",
            "    accuracy                           0.40      2867\n",
            "   macro avg       0.26      0.66      0.37      2867\n",
            "weighted avg       0.16      0.40      0.23      2867\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3445\n",
            "Precision: 0.1385\n",
            "Recall: 0.3445\n",
            "F1 Score: 0.1975\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      0.77      0.41        87\n",
            "     neutral       0.00      0.00      0.00       431\n",
            "    positive       0.38      0.90      0.53       199\n",
            "\n",
            "    accuracy                           0.34       717\n",
            "   macro avg       0.22      0.56      0.31       717\n",
            "weighted avg       0.14      0.34      0.20       717\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3618\n",
            "Precision: 0.1448\n",
            "Recall: 0.3618\n",
            "F1 Score: 0.2066\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      0.91      0.48        77\n",
            "     neutral       0.00      0.00      0.00       381\n",
            "    positive       0.38      0.91      0.54       175\n",
            "\n",
            "    accuracy                           0.36       633\n",
            "   macro avg       0.24      0.61      0.34       633\n",
            "weighted avg       0.14      0.36      0.21       633\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3617693522906793,\n",
              " 0.1447658026663404,\n",
              " 0.3617693522906793,\n",
              " 0.20657608985817316)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path = './glove.6B.300d.txt' # Testing with 50d for speed\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "filename = './Sentences_50Agree.txt'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, embedding_matrix = finance_preprocessing(filename, embeddings_index, embed_dim=300)\n",
        "\n",
        "train_data = SentimentDataset(X_train, y_train)\n",
        "val_data = SentimentDataset(X_val, y_val)\n",
        "test_data = SentimentDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "model_run = '300dim_50Agree_G2G'\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 2\n",
        "dropout = 0.75\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = EncoderGRU(hidden_size, embedding_matrix, gru_layers, dropout)\n",
        "decoder = DecoderGRU(hidden_size, gru_layers, dropout)\n",
        "g2g_model = GRU2GRU(encoder, decoder)\n",
        "\n",
        "trained_model = train_model(g2g_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print('\\nGRU2GRU Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(trained_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(trained_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(trained_model, test_loader, device, class_names)\n",
        "\n",
        "hidden_size = 81\n",
        "gru_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "lr=0.01\n",
        "epochs=20\n",
        "clip=0.9\n",
        "\n",
        "\n",
        "model_run = model_run = '300dim_50Agree_Basic'\n",
        "\n",
        "simple_model = SimpleClassifier(hidden_size, embedding_matrix, gru_layers=gru_layers, dropout=dropout)\n",
        "\n",
        "simple_model = train_model(simple_model, train_data, val_data, batch_size=batch_size, lr=lr, epochs=epochs, clip=clip, model_run=model_run)\n",
        "\n",
        "print('\\nBasic Model:\\n')\n",
        "\n",
        "print(\"Train Set Evaluation\")\n",
        "evaluate_model(simple_model, train_loader, device, class_names)\n",
        "\n",
        "print(\"Validation Set Evaluation\")\n",
        "evaluate_model(simple_model, val_loader, device, class_names)\n",
        "\n",
        "print(\"Test Set Evaluation\")\n",
        "evaluate_model(simple_model, test_loader, device, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXB2jSvp2XkM",
        "outputId": "3c322a21-4fa8-4cd9-943d-2a4777c5a2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Sentence length:  94\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:47<00:00,  2.18it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:03<00:00,  7.68it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Train Loss: 76.0778, Val Loss: 16.7431\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:49<00:00,  2.08it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:02<00:00,  9.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 - Train Loss: 70.7466, Val Loss: 16.2213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 103/103 [00:52<00:00,  1.95it/s]\n",
            "Validation Epoch 3/20: 100%|██████████| 26/26 [00:03<00:00,  7.61it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 - Train Loss: 71.2302, Val Loss: 15.9998\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 103/103 [00:49<00:00,  2.07it/s]\n",
            "Validation Epoch 4/20: 100%|██████████| 26/26 [00:02<00:00, 11.96it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 - Train Loss: 71.0042, Val Loss: 16.3600\n",
            "No improvement! Early stopping.\n",
            "\n",
            "GRU2GRU Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.2813\n",
            "Precision: 0.0791\n",
            "Recall: 0.2813\n",
            "F1 Score: 0.1235\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.28      1.00      0.44       927\n",
            "\n",
            "    accuracy                           0.28      3295\n",
            "   macro avg       0.09      0.33      0.15      3295\n",
            "weighted avg       0.08      0.28      0.12      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.2816\n",
            "Precision: 0.0793\n",
            "Recall: 0.2816\n",
            "F1 Score: 0.1237\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.28      1.00      0.44       232\n",
            "\n",
            "    accuracy                           0.28       824\n",
            "   macro avg       0.09      0.33      0.15       824\n",
            "weighted avg       0.08      0.28      0.12       824\n",
            "\n",
            "Test Set Evaluation\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2806\n",
            "Precision: 0.0787\n",
            "Recall: 0.2806\n",
            "F1 Score: 0.1230\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.28      1.00      0.44       204\n",
            "\n",
            "    accuracy                           0.28       727\n",
            "   macro avg       0.09      0.33      0.15       727\n",
            "weighted avg       0.08      0.28      0.12       727\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 103/103 [00:27<00:00,  3.77it/s]\n",
            "Validation Epoch 1/20: 100%|██████████| 26/26 [00:01<00:00, 14.00it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Train Loss: 51.2715, Val Loss: 8.9363\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 103/103 [00:26<00:00,  3.89it/s]\n",
            "Validation Epoch 2/20: 100%|██████████| 26/26 [00:01<00:00, 13.96it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 - Train Loss: 16.1854, Val Loss: 9.2881\n",
            "No improvement! Early stopping.\n",
            "\n",
            "Basic Model:\n",
            "\n",
            "Train Set Evaluation\n",
            "Accuracy: 0.4012\n",
            "Precision: 0.1686\n",
            "Recall: 0.4012\n",
            "F1 Score: 0.2354\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.54      0.97      0.69       410\n",
            "     neutral       0.00      0.00      0.00      1958\n",
            "    positive       0.36      1.00      0.53       927\n",
            "\n",
            "    accuracy                           0.40      3295\n",
            "   macro avg       0.30      0.66      0.41      3295\n",
            "weighted avg       0.17      0.40      0.24      3295\n",
            "\n",
            "Validation Set Evaluation\n",
            "Accuracy: 0.3629\n",
            "Precision: 0.1516\n",
            "Recall: 0.3629\n",
            "F1 Score: 0.2114\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      0.76      0.56       103\n",
            "     neutral       0.00      0.00      0.00       489\n",
            "    positive       0.34      0.95      0.50       232\n",
            "\n",
            "    accuracy                           0.36       824\n",
            "   macro avg       0.26      0.57      0.35       824\n",
            "weighted avg       0.15      0.36      0.21       824\n",
            "\n",
            "Test Set Evaluation\n",
            "Accuracy: 0.3453\n",
            "Precision: 0.1439\n",
            "Recall: 0.3453\n",
            "F1 Score: 0.1999\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.41      0.65      0.50        91\n",
            "     neutral       0.00      0.00      0.00       432\n",
            "    positive       0.33      0.94      0.49       204\n",
            "\n",
            "    accuracy                           0.35       727\n",
            "   macro avg       0.25      0.53      0.33       727\n",
            "weighted avg       0.14      0.35      0.20       727\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.3452544704264099,\n",
              " 0.14389813988464853,\n",
              " 0.3452544704264099,\n",
              " 0.19986252599942123)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "We see here there are very significant problems with our encoder-decoder model.\n",
        "\n",
        "Ultimately, we were unable to replicate or improve upon the results of Malo et al. We suspect that in order to improve upon the model that we developed a serious rework would be required adding additional information just as was done in the original paper.\n"
      ],
      "metadata": {
        "id": "ymCXY_jsAoYd"
      }
    }
  ]
}